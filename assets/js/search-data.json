{
  
    
        "post0": {
            "title": "Pre-Training with Surrogate Labels",
            "content": "1. Overview . In many real-world settings, the size of the labeled training sample is lower compared to the unlabeled test data. This blogpost demonstrates a technique that can improve the performance of neural networks in such settings by learning from both training and test data. This is done by pre-training a model on the complete data set using a surrogate label. The approach can help to reduce the impact of sampling bias by exposing the model to the test data and benefit from a larger sample size while learning. . We will focus on a computer vision application, but the idea can be used with deep learning models in other domains. We will use data from SIIM-ISIC Melanoma Classification Kaggle competition to distinguish malignant and bengin lesions on medical images. The modeling is performed in tensorflow. A shorter and interactive version of this blogpost is also available as a Kaggle notebook. . 2. Intuition . How to make use of the test sample on the pre-training stage? The labels are only observed for the training data. Luckily, in many settings, there is a bunch of meta-data available for both labeled and unlabeled images. Consider the task of lung cancer detection. The CT scans of cancer patients may contain information on the patient&#39;s age and gender. In contrast with the label, which requires medical tests or experts&#39; diagnosis, meta-data is available at no additional cost. Another example is bird image classification, where the image meta-data such as time and location of the photo can serve the same purpose. In this blogpost, we will focus on malignant lesion classification, where patient meta-data is available for all images. . We can leverage meta-data in the following way: . Pre-train a supplementary model on the complete train + test data using one of the meta-features as a surrogate label. | Initialize from the pre-trained weights when training the main model. | The intuition behind this approach is that by learning to classify images according to one of meta variables, the model can learn some of the visual features that might be useful for the main task, which in our case is malignant lesion classification. For instance, lesion size and skin color can be helpful in determining both lesion location (surrogate label) and lesion type (actual label). Exposing the model to the test data also allows it to take a sneak peek at test images, which may help to learn patterns prevalent in the test distribution. . P.S. The notebook hevaily relies on the great modeling pipeline developed by Chris Deotte for the SIIM-ISIC competition and reuses much of his original code. Kindly refer to his notebook for general questions on the pipeline where he provided comments and documentation. . 3. Initialization . #collapse-hide ### PACKAGES !pip install -q efficientnet &gt;&gt; /dev/null import pandas as pd, numpy as np from kaggle_datasets import KaggleDatasets import tensorflow as tf, re, math import tensorflow.keras.backend as K import efficientnet.tfkeras as efn from sklearn.model_selection import KFold from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt from scipy.stats import rankdata import PIL, cv2 . . Let&#39;s set up training parameters such as image size, number of folds and batch size. In addition to these parameters, we introduce USE_PRETRAIN_WEIGHTS variable to reflect whether we want to pre-train a supplementary model on full data before training the main melanoma classification model. . For demonstartion purposes, we use EfficientNet B0, 128x128 image size and no TTA. Feel free to experiment with larger architectures and images sizes by editing this notebook. . #collapse-show # DEVICE DEVICE = &quot;TPU&quot; # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD SEED = 42 # NUMBER OF FOLDS. USE 3, 5, OR 15 FOLDS = 5 # WHICH IMAGE SIZES TO LOAD EACH FOLD IMG_SIZES = [128]*FOLDS # BATCH SIZE AND EPOCHS BATCH_SIZES = [32]*FOLDS EPOCHS = [10]*FOLDS # WHICH EFFICIENTNET TO USE EFF_NETS = [0]*FOLDS # WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST WGTS = [1/FOLDS]*FOLDS # PRETRAINED WEIGHTS USE_PRETRAIN_WEIGHTS = True . . Below, we connect to TPU or GPU for faster training. . #collapse-hide # CONNECT TO DEVICE if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTO = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . . connecting to TPU... Running on TPU grpc://10.0.0.2:8470 initializing TPU ... TPU initialized REPLICAS: 8 . 4. Image processing . First, we specify data paths. The data is stored as tfrecords to enable fast processing. You can read more on the data here. . #collapse-show # IMAGE PATHS GCS_PATH = [None]*FOLDS for i,k in enumerate(IMG_SIZES): GCS_PATH[i] = KaggleDatasets().get_gcs_path(&#39;melanoma-%ix%i&#39;%(k,k)) files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;))) files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + &#39;/test*.tfrec&#39;))) . . The read_labeled_tfrecord() function provides two outputs: . Image tensor. | Either anatom_site_general_challenge or target as a label. The former is a one-hot-encoded categorical feature with six possible values indicating the lesion location. The latter is a binary target indicating whether the lesion is malgnant. The selection of the label is controlled by the pretraining argument read from the get_dataset() function below. Setting pretraining = True implies using anatom_site_general_challenge as a surrogate label. | We also set up read_unlabeled_tfrecord() that returns image and image name. . #collapse-show def read_labeled_tfrecord(example, pretraining = False): if pretraining: tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), &#39;anatom_site_general_challenge&#39;: tf.io.FixedLenFeature([], tf.int64), } else: tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), &#39;target&#39; : tf.io.FixedLenFeature([], tf.int64) } example = tf.io.parse_single_example(example, tfrec_format) return example[&#39;image&#39;], tf.one_hot(example[&#39;anatom_site_general_challenge&#39;], 6) if pretraining else example[&#39;target&#39;] def read_unlabeled_tfrecord(example, return_image_name=True): tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), } example = tf.io.parse_single_example(example, tfrec_format) return example[&#39;image&#39;], example[&#39;image_name&#39;] if return_image_name else 0 def prepare_image(img, dim = 256): img = tf.image.decode_jpeg(img, channels = 3) img = tf.cast(img, tf.float32) / 255.0 img = img * circle_mask img = tf.reshape(img, [dim,dim, 3]) return img def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) . . The get_dataset() function is a wrapper function that loads and processes images given the arguments that control the import options. . #collapse-show def get_dataset(files, shuffle = False, repeat = False, labeled = True, pretraining = False, return_image_names = True, batch_size = 16, dim = 256): ds = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO) ds = ds.cache() if repeat: ds = ds.repeat() if shuffle: ds = ds.shuffle(1024*2) #if too large causes OOM in GPU CPU opt = tf.data.Options() opt.experimental_deterministic = False ds = ds.with_options(opt) if labeled: ds = ds = ds.map(lambda example: read_labeled_tfrecord(example, pretraining), num_parallel_calls=AUTO) else: ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), num_parallel_calls = AUTO) ds = ds.map(lambda img, imgname_or_label: ( prepare_image(img, dim = dim), imgname_or_label), num_parallel_calls = AUTO) ds = ds.batch(batch_size * REPLICAS) ds = ds.prefetch(AUTO) return ds . . We also use a circular crop (a.k.a. microscope augmentation) to improve image consistency. The snippet below creates a circular mask, which is applied in the prepare_image() function. . #collapse-show # CIRCLE CROP PREPARATIONS circle_img = np.zeros((IMG_SIZES[0], IMG_SIZES[0]), np.uint8) circle_img = cv2.circle(circle_img, (int(IMG_SIZES[0]/2), int(IMG_SIZES[0]/2)), int(IMG_SIZES[0]/2), 1, thickness = -1) circle_img = np.repeat(circle_img[:, :, np.newaxis], 3, axis = 2) circle_mask = tf.cast(circle_img, tf.float32) . . Let&#39;s have a quick look at a batch of our images: . #collapse-hide # LOAD DATA AND APPLY AUGMENTATIONS def show_dataset(thumb_size, cols, rows, ds): mosaic = PIL.Image.new(mode=&#39;RGB&#39;, size=(thumb_size*cols + (cols-1), thumb_size*rows + (rows-1))) for idx, data in enumerate(iter(ds)): img, target_or_imgid = data ix = idx % cols iy = idx // cols img = np.clip(img.numpy() * 255, 0, 255).astype(np.uint8) img = PIL.Image.fromarray(img) img = img.resize((thumb_size, thumb_size), resample = PIL.Image.BILINEAR) mosaic.paste(img, (ix*thumb_size + ix, iy*thumb_size + iy)) nn = target_or_imgid.numpy().decode(&quot;utf-8&quot;) display(mosaic) return nn files_train = tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;) ds = tf.data.TFRecordDataset(files_train, num_parallel_reads = AUTO).shuffle(1024) ds = ds.take(10).cache() ds = ds.map(read_unlabeled_tfrecord, num_parallel_calls = AUTO) ds = ds.map(lambda img, target: (prepare_image(img, dim = IMG_SIZES[0]), target), num_parallel_calls = AUTO) ds = ds.take(12*5) ds = ds.prefetch(AUTO) # DISPLAY IMAGES name = show_dataset(128, 5, 2, ds) . . 5. Modeling . Pre-trained model with surrogate label . The build_model() function incorporates three important features that depend on the training regime: . When building a model for pre-training, we use CategoricalCrossentropy as a loss because anatom_site_general_challenge is a categorical variable. When building a model that classifies lesions as benign/malgnant, we use BinaryCrossentropy as a loss. | When training a final binary classification model, we load the pre-trained weights using base.load_weights(&#39;base_weights.h5&#39;) if use_pretrain_weights == True. | We use a dense layer with six output nodes and softmax activation when doing pre-training and a dense layer with a single output node and sigmoid activation when training a final model. | #collapse-show EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7] def build_model(dim = 256, ef = 0, pretraining = False, use_pretrain_weights = False): # base inp = tf.keras.layers.Input(shape = (dim,dim,3)) base = EFNS[ef](input_shape = (dim,dim,3), weights = &#39;imagenet&#39;, include_top = False) # base weights if use_pretrain_weights: base.load_weights(&#39;base_weights.h5&#39;) x = base(inp) x = tf.keras.layers.GlobalAveragePooling2D()(x) if pretraining: x = tf.keras.layers.Dense(6, activation = &#39;softmax&#39;)(x) model = tf.keras.Model(inputs = inp, outputs = x) opt = tf.keras.optimizers.Adam(learning_rate = 0.001) loss = tf.keras.losses.CategoricalCrossentropy() model.compile(optimizer = opt, loss = loss) else: x = tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)(x) model = tf.keras.Model(inputs = inp, outputs = x) opt = tf.keras.optimizers.Adam(learning_rate = 0.001) loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.01) model.compile(optimizer = opt, loss = loss, metrics = [&#39;AUC&#39;]) return model . . #collapse-hide ### LEARNING RATE SCHEDULE def get_lr_callback(batch_size=8): lr_start = 0.000005 lr_max = 0.00000125 * REPLICAS * batch_size lr_min = 0.000001 lr_ramp_ep = 5 lr_sus_ep = 0 lr_decay = 0.8 def lrfn(epoch): if epoch &lt; lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start elif epoch &lt; lr_ramp_ep + lr_sus_ep: lr = lr_max else: lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min return lr lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False) return lr_callback . . The pre-trained model is trained on both training and test data. Here, we use the original training data merged with the complete test set as a training sample. We fix the number of training epochs to EPOCHS and do not perform early stopping. You can also experiment with setting up a small validation sample from both training and test data to perform early stopping. . #collapse-show ### PRE-TRAINED MODEL if USE_PRETRAIN_WEIGHTS: # USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit VERBOSE = 2 # DISPLAY INFO if DEVICE == &#39;TPU&#39;: if tpu: tf.tpu.experimental.initialize_tpu_system(tpu) # CREATE TRAIN AND VALIDATION SUBSETS files_train = tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;) print(&#39;#### Using 2020 train data&#39;) files_train += tf.io.gfile.glob(GCS_PATH[0] + &#39;/test*.tfrec&#39;) print(&#39;#### Using 2020 test data&#39;) np.random.shuffle(files_train) # BUILD MODEL K.clear_session() tf.random.set_seed(SEED) with strategy.scope(): model = build_model(dim = IMG_SIZES[0], ef = EFF_NETS[0], pretraining = True) # SAVE BEST MODEL EACH FOLD sv = tf.keras.callbacks.ModelCheckpoint( &#39;weights.h5&#39;, monitor=&#39;loss&#39;, verbose=0, save_best_only=True, save_weights_only=True, mode=&#39;min&#39;, save_freq=&#39;epoch&#39;) # TRAIN print(&#39;Training...&#39;) history = model.fit( get_dataset(files_train, dim = IMG_SIZES[0], batch_size = BATCH_SIZES[0], shuffle = True, repeat = True, pretraining = True), epochs = EPOCHS[0], callbacks = [sv, get_lr_callback(BATCH_SIZES[0])], steps_per_epoch = count_data_items(files_train)/BATCH_SIZES[0]//REPLICAS, verbose = VERBOSE) else: print(&#39;#### NOT using a pre-trained model&#39;) . . #### Using 2020 train data #### Using 2020 test data Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5 16809984/16804768 [==============================] - 0s 0us/step Training... Epoch 1/10 170/170 - 10s - loss: 1.7556 - lr: 5.0000e-06 Epoch 2/10 170/170 - 10s - loss: 1.1257 - lr: 6.8000e-05 Epoch 3/10 170/170 - 11s - loss: 0.8906 - lr: 1.3100e-04 Epoch 4/10 170/170 - 10s - loss: 0.8118 - lr: 1.9400e-04 Epoch 5/10 170/170 - 9s - loss: 0.8222 - lr: 2.5700e-04 Epoch 6/10 170/170 - 9s - loss: 0.8626 - lr: 3.2000e-04 Epoch 7/10 170/170 - 9s - loss: 0.8402 - lr: 2.5620e-04 Epoch 8/10 170/170 - 9s - loss: 0.8257 - lr: 2.0516e-04 Epoch 9/10 170/170 - 10s - loss: 0.8091 - lr: 1.6433e-04 Epoch 10/10 170/170 - 10s - loss: 0.7865 - lr: 1.3166e-04 . The pre-training is complete! Now, we need to resave weights of our pre-trained model to make it easier to load them in the future. We are not really interested in the classification head, so we only export the weights of the convolutional part of the network. We can index these layers using model.layers[1]. . #collapse-show # LOAD WEIGHTS AND CHECK MODEL if USE_PRETRAIN_WEIGHTS: model.load_weights(&#39;weights.h5&#39;) model.summary() # EXPORT BASE WEIGHTS if USE_PRETRAIN_WEIGHTS: model.layers[1].save_weights(&#39;base_weights.h5&#39;) . . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 128, 128, 3)] 0 _________________________________________________________________ efficientnet-b0 (Model) (None, 4, 4, 1280) 4049564 _________________________________________________________________ global_average_pooling2d (Gl (None, 1280) 0 _________________________________________________________________ dense (Dense) (None, 6) 7686 ================================================================= Total params: 4,057,250 Trainable params: 4,015,234 Non-trainable params: 42,016 _________________________________________________________________ . Main classification model . Now we can train a final classification model using a cross-validation framework on the training data! . We need to take care of a couple of changes: . Make sure that we don&#39;t use test data in the training folds. | Set use_pretrain_weights = True and pretraining = False in the build_model() function to initialize from the pre-trained weights in the beginning of each fold. | #collapse-show # USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit VERBOSE = 0 skf = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED) oof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] preds = np.zeros((count_data_items(files_test),1)) for fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))): # DISPLAY FOLD INFO if DEVICE == &#39;TPU&#39;: if tpu: tf.tpu.experimental.initialize_tpu_system(tpu) print(&#39;#&#39;*25); print(&#39;#### FOLD&#39;,fold+1) # CREATE TRAIN AND VALIDATION SUBSETS files_train = tf.io.gfile.glob([GCS_PATH[fold] + &#39;/train%.2i*.tfrec&#39;%x for x in idxT]) np.random.shuffle(files_train); print(&#39;#&#39;*25) files_valid = tf.io.gfile.glob([GCS_PATH[fold] + &#39;/train%.2i*.tfrec&#39;%x for x in idxV]) files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + &#39;/test*.tfrec&#39;))) # BUILD MODEL K.clear_session() tf.random.set_seed(SEED) with strategy.scope(): model = build_model(dim = IMG_SIZES[fold], ef = EFF_NETS[fold], use_pretrain_weights = USE_PRETRAIN_WEIGHTS, pretraining = False) # SAVE BEST MODEL EACH FOLD sv = tf.keras.callbacks.ModelCheckpoint( &#39;fold-%i.h5&#39;%fold, monitor=&#39;val_auc&#39;, verbose=0, save_best_only=True, save_weights_only=True, mode=&#39;max&#39;, save_freq=&#39;epoch&#39;) # TRAIN print(&#39;Training...&#39;) history = model.fit( get_dataset(files_train, shuffle = True, repeat = True, dim = IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), epochs = EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], steps_per_epoch = count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS, validation_data = get_dataset(files_valid, shuffle = False, repeat = False, dim = IMG_SIZES[fold]), verbose = VERBOSE ) model.load_weights(&#39;fold-%i.h5&#39;%fold) # PREDICT OOF print(&#39;Predicting OOF...&#39;) ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4) ct_valid = count_data_items(files_valid); STEPS = ct_valid/BATCH_SIZES[fold]/4/REPLICAS pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:ct_valid,] oof_pred.append(pred) # GET OOF TARGETS AND NAMES ds_valid = get_dataset(files_valid,dim=IMG_SIZES[fold],labeled=True, return_image_names=True) oof_tar.append(np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) ) oof_folds.append(np.ones_like(oof_tar[-1],dtype=&#39;int8&#39;)*fold ) ds = get_dataset(files_valid,dim=IMG_SIZES[fold],labeled=False,return_image_names=True) oof_names.append(np.array([img_name.numpy().decode(&quot;utf-8&quot;) for img, img_name in iter(ds.unbatch())])) # PREDICT TEST print(&#39;Predicting Test...&#39;) ds_test = get_dataset(files_test,labeled=False,return_image_names=False,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4) ct_test = count_data_items(files_test); STEPS = ct_test/BATCH_SIZES[fold]/4/REPLICAS pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:ct_test,] preds[:,0] += (pred * WGTS[fold]).reshape(-1) . . ######################### #### FOLD 1 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 2 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 3 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 4 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 5 ######################### Training... Predicting OOF... Predicting Test... . #collapse-show # COMPUTE OOF AUC oof = np.concatenate(oof_pred); true = np.concatenate(oof_tar); names = np.concatenate(oof_names); folds = np.concatenate(oof_folds) auc = roc_auc_score(true,oof) print(&#39;Overall OOF AUC = %.4f&#39;%auc) . . Overall OOF AUC = 0.8414 . How does the OOF AUC compare to a model without the pre-training stage? To check this, we can simply set USE_PRETRAIN_WEIGHTS = False in the begining of the notebeook. This is done in thus version of the Kaggle notebook, yielding a model with a lower OOF AUC (0.8329 compared to 0.8414 with pre-training). . Compared to a model initialized from the Imagenet weights, pre-training on a surrogate label brings a CV improvement. The AUC gain also translates into the performance gain on the competition leaderboard (increase from 0.8582 to 0.8809). Great news! . 6. Closing words . This is the end of this blogpost. Using a computer vision application, we demonstrated how to use meta-data to construct a surrogate label and pre-train a CNN on both training and test data to improve performance. . The pre-trained model can be further optimized to increase performance gains. Using a validation subset on the pre-training stage can help to tune the number of epochs and other learning parameters. Another idea could be to construct a surrogate label with more unique values (e.g., combination of anatom_site_general_challenge and sex) to make the pre-training task more challenging and motivate the model to learn better. On the other hand, further optimizing the main classification model may reduce the benefit of pre-training. .",
            "url": "https://kozodoi.me/python/deep%20learning/computer%20vision/competitions/2020/08/30/pre-training.html",
            "relUrl": "/python/deep%20learning/computer%20vision/competitions/2020/08/30/pre-training.html",
            "date": " â€¢ Aug 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Mining Cup 2020 Solution",
            "content": "1. Overview . Data Mining Cup is an annual international machine learning competition. The 2020 edition was devoted to demand forecasting. Together with Elizaveta Zinovyeva, we represented the Humboldt University of Berlin and finished in the top-15 of the leaderboard. . Demand forecasting is an important task that helps to optimize inventory planning. The optimized stocks can reduce retailer&#39;s costs and increase customer satisfaction due to faster delivery time. The competition task was to predict demand for a set of items using past purchase data. . This blogpost provides a detailed walkthrough covering the crucial steps of our solution: . data preparation and feature engineering | aggregation of transactional data into the daily format | implementation of custom profit-driven loss functions | two-stage demand forecasting with LightGBM | hyper-parameter tuning with hyperopt | . Feel free to jump directly to the parts that are interesting to you! The complete notebooks reproducing our solution are available on Github. . 2. Data preparation . Data overview . The competition data consists of three data sets: . infos.csv: prices and promotions in the unlabeled test set | items.csv: item-specific characteristics such as brand, manufacturer, etc | orders.csv: purchase transactions over the 6-month period | . Let&#39;s have a look at the data: . #collapse-show # packages import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # data import infos = pd.read_csv(&#39;../data/raw/infos.csv&#39;, sep = &#39;|&#39;) items = pd.read_csv(&#39;../data/raw/items.csv&#39;, sep = &#39;|&#39;) orders = pd.read_csv(&#39;../data/raw/orders.csv&#39;, sep = &#39;|&#39;) print(infos.shape) print(items.shape) print(orders.shape) . . (10463, 3) (10463, 8) (2181955, 5) . #collapse-show infos.head() . . itemID simulationPrice promotion . 0 | 1 | 3.43 | NaN | . 1 | 2 | 9.15 | NaN | . 2 | 3 | 14.04 | NaN | . 3 | 4 | 14.10 | NaN | . 4 | 5 | 7.48 | NaN | . #collapse-show items.head() . . itemID brand manufacturer customerRating category1 category2 category3 recommendedRetailPrice . 0 | 1 | 0 | 1 | 4.38 | 1 | 1 | 1 | 8.84 | . 1 | 2 | 0 | 2 | 3.00 | 1 | 2 | 1 | 16.92 | . 2 | 3 | 0 | 3 | 5.00 | 1 | 3 | 1 | 15.89 | . 3 | 4 | 0 | 2 | 4.44 | 1 | 2 | 1 | 40.17 | . 4 | 5 | 0 | 2 | 2.33 | 1 | 1 | 1 | 17.04 | . #collapse-show orders.head() . . time transactID itemID order salesPrice . 0 | 2018-01-01 00:01:56 | 2278968 | 450 | 1 | 17.42 | . 1 | 2018-01-01 00:01:56 | 2278968 | 83 | 1 | 5.19 | . 2 | 2018-01-01 00:07:11 | 2255797 | 7851 | 2 | 20.47 | . 3 | 2018-01-01 00:09:24 | 2278968 | 450 | 1 | 17.42 | . 4 | 2018-01-01 00:09:24 | 2278968 | 83 | 1 | 5.19 | . For each of the 10,463 items, we need to predict the total number of orders in the 14-day period following the last day in orders. . Preprocessing . Let&#39;s do some preprocessing! First, we merge items and infos that contain item-level data: . #collapse-show print(infos.shape) print(items.shape) items = pd.merge(infos, items, on = &#39;itemID&#39;, how = &#39;left&#39;) print(items.shape) del infos . . (10463, 3) (10463, 8) (10463, 10) . Next, we check and convert feature types to the appropriate format: . #collapse-hide print(&#39;-&#39; * 50) print(items.dtypes) print(&#39;-&#39; * 50) print(orders.dtypes) print(&#39;-&#39; * 50) # items for var in [&#39;itemID&#39;, &#39;brand&#39;, &#39;manufacturer&#39;, &#39;category1&#39;, &#39;category2&#39;, &#39;category3&#39;]: items[var] = items[var].astype(&#39;str&#39;).astype(&#39;object&#39;) # orders for var in [&#39;transactID&#39;, &#39;itemID&#39;]: orders[var] = orders[var].astype(&#39;str&#39;).astype(&#39;object&#39;) # dates orders[&#39;time&#39;] = pd.to_datetime(orders[&#39;time&#39;].astype(&#39;str&#39;), infer_datetime_format = True) . . -- itemID int64 simulationPrice float64 promotion object brand int64 manufacturer int64 customerRating float64 category1 int64 category2 int64 category3 int64 recommendedRetailPrice float64 dtype: object -- time object transactID int64 itemID int64 order int64 salesPrice float64 dtype: object -- . Finally, we unfold the promotion feature containing a sequence of coma-separated promotion dates. We use split_nested_features() from dptools to split a string column into separate features. . dptools is a package developed by me to simplify some of the common data preprocessing and feature engineering tasks. Below, you will see more examples on using dptools for other applications. You can read more about the package here. . #collapse-show # import packages !pip install dptools from dptools import * # split promotion feature items = split_nested_features(items, split_vars = &#39;promotion&#39;, sep = &#39;,&#39;) print(items.head()) # convert dates promotion_vars = items.filter(like = &#39;promotion_&#39;).columns for var in promotion_vars: items[var] = pd.to_datetime(items[var], infer_datetime_format = True) . . Added 3 split-based features. . itemID simulationPrice brand manufacturer customerRating category1 category2 category3 recommendedRetailPrice promotion_0 promotion_1 promotion_2 . 0 | 1 | 3.43 | NaN | 1 | 4.38 | 1 | 1 | 1 | 8.84 | NaN | NaN | NaN | . 1 | 2 | 9.15 | NaN | 2 | 3.00 | 1 | 2 | 1 | 16.92 | NaN | NaN | NaN | . 2 | 3 | 14.04 | NaN | 3 | 5.00 | 1 | 3 | 1 | 15.89 | NaN | NaN | NaN | . 3 | 4 | 14.10 | NaN | 2 | 4.44 | 1 | 2 | 1 | 40.17 | NaN | NaN | NaN | . 4 | 5 | 7.48 | NaN | 2 | 2.33 | 1 | 1 | 1 | 17.04 | NaN | NaN | NaN | . We can now export the data as csv. I use save_csv_version() that automatically adds a version number to the file name to prevent overwriting the data after making changes in the code. . #collapse-show save_csv_version(&#39;../data/prepared/orders.csv&#39;, orders, index = False, compression = &#39;gzip&#39;) save_csv_version(&#39;../data/prepared/items.csv&#39;, items, index = False, compression = &#39;gzip&#39;) print(orders.shape) print(items.shape) . . Saved as ../data/prepared/orders_v2.csv Saved as ../data/prepared/items_v2.csv (2181955, 5) (10463, 12) . 3. Aggregation and feature engineering . Data aggregation . Let&#39;s work with orders, which is formatted as a list of transactions with timestamps. We need to aggregate this data in order to use it for the modeling. . Since the task is a 14-day demand forecasting, a simple way forward would be to aggregate transactions on a two-week basis. However, this could lead to losing some more granular information. Instead, we aggregate transactions by day: . #collapse-show orders[&#39;day_of_year&#39;] = orders[&#39;time&#39;].dt.dayofyear orders_price = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;])[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() orders = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;])[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() orders.head() . . itemID day_of_year order . 0 | 1 | 23 | 1 | . 1 | 1 | 25 | 1 | . 2 | 1 | 29 | 307 | . 3 | 1 | 30 | 3 | . 4 | 1 | 31 | 1 | . Adding missing item-day combinations . The aggregated data only contains entries for day-item pairs for which there is at least one transaction. This results in missing information: . most items are only sold on a few days; no data on days with no orders is recorded | there are a few items that are never sold and therefore do not appear in orders | . To account for the missing data, we add entries with order = 0 for missing day-item combinations including the items that were never sold. This increases the number of observations from 100,771 to 1,883,340 and provides useful information about zero sales. . #collapse-show # add items that were never sold before missing_itemIDs = set(items[&#39;itemID&#39;].unique()) - set(orders[&#39;itemID&#39;].unique()) missing_rows = pd.DataFrame({&#39;itemID&#39;: list(missing_itemIDs), &#39;day_of_year&#39;: np.ones(len(missing_itemIDs)).astype(&#39;int&#39;), &#39;order&#39;: np.zeros(len(missing_itemIDs)).astype(&#39;int&#39;)}) orders = pd.concat([orders, missing_rows], axis = 0) print(orders.shape) # add zeros for days with no transactions agg_orders = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;]).order.unique().unstack(&#39;day_of_year&#39;).stack(&#39;day_of_year&#39;, dropna = False) agg_orders = agg_orders.reset_index() agg_orders.columns = [&#39;itemID&#39;, &#39;day_of_year&#39;, &#39;order&#39;] agg_orders[&#39;order&#39;].fillna(0, inplace = True) agg_orders[&#39;order&#39;] = agg_orders[&#39;order&#39;].astype(int) print(agg_orders.shape) . . (100771, 3) (1883340, 3) . Labeling promotions . The documentation says that promotions in the training data are not explicitly marked. . We need to manually mark promotion days. Ignoring it complicates forecasting because the number of orders in some days explodes without an apparent reason. In such cases, the underlying reason is likely to be a promotion, which should be reflected in a corresponding feature. . We need to be very careful and conservative about marking promotions. Labeling too many days as promotions based on the number of orders risks introducing data leakage since the number of orders is unknown at the prediction time. Below, I use find_peaks() to isolate peaks in the order time series and encode them as promotions: . #collapse-show # computations agg_orders[&#39;promotion&#39;] = 0 for itemID in tqdm(agg_orders[&#39;itemID&#39;].unique()): promo = np.zeros(len(agg_orders[agg_orders[&#39;itemID&#39;] == itemID])) avg = agg_orders[(agg_orders[&#39;itemID&#39;] == itemID)][&#39;order&#39;].median() std = agg_orders[(agg_orders[&#39;itemID&#39;] == itemID)][&#39;order&#39;].std() peaks, _ = find_peaks(np.append(agg_orders[agg_orders[&#39;itemID&#39;] == itemID][&#39;order&#39;].values, avg), # append avg to enable marking last point as promo prominence = max(5, std), # peak difference with neighbor points; max(5,std) to exclude cases when std is too small height = avg + 2*std) # minimal height of a peak promo[peaks] = 1 agg_orders.loc[agg_orders[&#39;itemID&#39;] == itemID, &#39;promotion&#39;] = promo # compare promotion number promo_in_train = (agg_orders[&#39;promotion&#39;].sum() / agg_orders[&#39;day_of_year&#39;].max()) / len(items) promo_in_test = (3*len(items) - items.promotion_0.isnull().sum() - items.promotion_2.isnull().sum() - items.promotion_1.isnull().sum()) / 14 / len(items) print(&#39;Daily p(promotion) per item in train: {}&#39;.format(np.round(promo_in_train, 4))) print(&#39;Daily p(promotion) per item in test: {}&#39;.format(np.round(promo_in_test , 4))) . . Daily p(promotion) per item in train: 0.0079 Daily p(promotion) per item in test: 0.0141 . Our method identifies 14,911 promotions. Compared to the unlabeled test set where promotions are explicitly reported, this amounts to about twice as few promotions per item and day. . Let&#39;s look at some items to check which observations are marked as promotions: . #collapse-hide # compute promo count promo_count = agg_orders.groupby(&#39;itemID&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() promo_count = promo_count.sort_values(&#39;promotion&#39;).reset_index(drop = True) # plot some items item_plots = [0, 2000, 4000, 6000, 8000, 9000, 10000, 10100, 10200, 10300, 10400, 10462] fig = plt.figure(figsize = (16, 12)) for i in range(len(item_plots)): plt.subplot(3, 4, i + 1) df = agg_orders[agg_orders.itemID == promo_count[&#39;itemID&#39;][item_plots[i]]] plt.scatter(df[&#39;day_of_year&#39;], df[&#39;order&#39;], c = df[&#39;promotion&#39;]) plt.ylabel(&#39;Total Orders&#39;) plt.xlabel(&#39;Day&#39;) . . . The yellow marker indicates promotions. Our method identifies some outliers as promotions but misses a few points that are less prominent. At the same time, we can not be sure that these cases are necessarily promotions: the large number of orders on these days could be observed due to other reasons. We will stick to this identification method but note that this aspect might require further improvement. . Feature engineering . Now that the data is aggregated, we can construct transaction-based features as well as the targets. For each day, we compute target as the total number of orders in the following 14 days. The days preceeding the considered day are used to extract features. For each day, we extract slices of the past [1, 7, ..., 35] days and compute features based on data from that slice. . For each item, we construct the following features: . the total count of orders and ordered items | the total count of promotions | mean item price | recency of the last order | . The number of orders and promotions is also aggregated on a manufacturer and category level. . In addition, we use tsfresh package to automatically extract features based on the order dynamics in the last 35 days. tsfresh computes hundreds of features describing the time series. We only keep features with no missing values for all day-item combinations. . Finally, we compute features based on the two-week period for which we predict demand: the number of promotions and mean prices per item, manufacturer and category. . #collapse-show # packages from tsfresh import extract_features # parameters days_input = [1, 7, 14, 21, 28, 35] days_target = 14 # preparations day_first = np.max(days_input) day_last = agg_orders[&#39;day_of_year&#39;].max() - days_target + 1 orders = None # merge manufacturer and category agg_orders = agg_orders.merge(items[[&#39;itemID&#39;, &#39;manufacturer&#39;]], how = &#39;left&#39;) agg_orders = agg_orders.merge(items[[&#39;itemID&#39;, &#39;category&#39;]], how = &#39;left&#39;) # computations for day_of_year in tqdm(list(range(149, day_last)) + [agg_orders[&#39;day_of_year&#39;].max()]): ###### VALIDAION: TARGET, PROMOTIONS, PRICES # day intervals target_day_min = day_of_year + 1 target_day_max = day_of_year + days_target # compute target and promo: labeled data if day_of_year &lt; agg_orders[&#39;day_of_year&#39;].max(): # target and future promo tmp_df = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;itemID&#39;)[&#39;order&#39;, &#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df.columns = [&#39;itemID&#39;, &#39;target&#39;, &#39;promo_in_test&#39;] # future price tmp_df[&#39;mean_price_test&#39;] = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;itemID&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index()[&#39;salesPrice&#39;] # merge manufacturer and category tmp_df = tmp_df.merge(items[[&#39;itemID&#39;, &#39;manufacturer&#39;, &#39;category&#39;]], how = &#39;left&#39;, on = &#39;itemID&#39;) # future price per manufacturer tmp_df_manufacturer = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;manufacturer&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_price_test_manufacturer&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future price per category tmp_df_category = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;category&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;mean_price_test_category&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # future promo per manufacturer tmp_df_manufacturer = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;manufacturer&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;promo_in_test_manufacturer&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future promo per category tmp_df_category = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;category&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;promo_in_test_category&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # compute target and promo: unlabeled data else: # placeholders tmp_df = pd.DataFrame({&#39;itemID&#39;: items.itemID, &#39;target&#39;: np.nan, &#39;promo_in_test&#39;: np.nan, &#39;mean_price_test&#39;: items.simulationPrice, &#39;manufacturer&#39;: items.manufacturer, &#39;category&#39;: items.category, &#39;promo_in_test_manufacturer&#39;: np.nan, &#39;promo_in_test_category&#39;: np.nan}) ###### TRAINING: LAG-BASED FEATURES # compute features for day_input in days_input: # day intervals input_day_min = day_of_year - day_input + 1 input_day_max = day_of_year # frequency, promo and price tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;itemID&#39;) tmp_df[&#39;order_sum_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index()[&#39;order&#39;] tmp_df[&#39;order_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df[&#39;promo_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df[&#39;mean_price_last_&#39; + str(day_input)] = tmp_df_input[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index()[&#39;salesPrice&#39;] # frequency, promo per manufacturer tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;manufacturer&#39;) tmp_df_manufacturer = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;order_manufacturer_sum_last_&#39; + str(day_input)] tmp_df_manufacturer[&#39;order_manufacturer_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df_manufacturer[&#39;promo_manufacturer_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # frequency, promo per category tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;category&#39;) tmp_df_category = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;order_category_sum_last_&#39; + str(day_input)] tmp_df_category[&#39;order_category_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df_category[&#39;promo_category_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # frequency, promo per all items tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max)] tmp_df[&#39;order_all_sum_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;) tmp_df[&#39;order_all_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])) tmp_df[&#39;promo_all_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;) # recency if day_input == max(days_input): tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) &amp; (agg_orders[&#39;order&#39;] &gt; 0) ].groupby(&#39;itemID&#39;) tmp_df[&#39;days_since_last_order&#39;] = (day_of_year - tmp_df_input[&#39;day_of_year&#39;].agg(&#39;max&#39;)).reindex(tmp_df.itemID).reset_index()[&#39;day_of_year&#39;] tmp_df[&#39;days_since_last_order&#39;].fillna(day_input, inplace = True) # tsfresh features if day_input == max(days_input): tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max)] tmp_df_input = tmp_df_input[[&#39;day_of_year&#39;, &#39;itemID&#39;, &#39;order&#39;]] extracted_features = extract_features(tmp_df_input, column_id = &#39;itemID&#39;, column_sort = &#39;day_of_year&#39;) extracted_features[&#39;itemID&#39;] = extracted_features.index tmp_df = tmp_df.merge(extracted_features, how = &#39;left&#39;, on = &#39;itemID&#39;) ###### FINAL PREPARATIONS # add day of year tmp_df.insert(1, column = &#39;day_of_year&#39;, value = day_of_year) # merge data orders = pd.concat([orders, tmp_df], axis = 0) # drop manufacturer and category del orders[&#39;manufacturer&#39;] del orders[&#39;category&#39;] ##### REMOVE MISSINGS good_nas = [&#39;target&#39;, &#39;mean_price_test_category&#39;, &#39;mean_price_test_manufacturer&#39;, &#39;promo_in_test&#39;, &#39;promo_in_test_category&#39;, &#39;promo_in_test_manufacturer&#39;] nonas = list(orders.columns[orders.isnull().sum() == 0]) + good_nas orders = orders[nonas] print(orders.shape) ##### COMPUTE MEAN PRICE RATIOS print(orders.shape) price_vars = [&#39;mean_price_last_1&#39;, &#39;mean_price_last_7&#39;, &#39;mean_price_last_14&#39;, &#39;mean_price_last_21&#39;, &#39;mean_price_last_28&#39;, &#39;mean_price_last_35&#39;] for var in price_vars: orders[&#39;ratio_&#39; + str(var)] = orders[&#39;mean_price_test&#39;] / orders[var] orders[&#39;ratio_manufacturer_&#39; + str(var)] = orders[&#39;mean_price_test_manufacturer&#39;] / orders[var] orders[&#39;ratio_category_&#39; + str(var)] = orders[&#39;mean_price_test_category&#39;] / orders[var] print(orders.shape) . . (1391579, 458) (1391579, 470) . The feature extraction takes about ten hours and outputs a data set with 470 features. Great job! . Now, let&#39;s create features in the items data frame: . ratio of the actual and recommended price | item category index constructed of three subcategories | customer rating realtive to the average rating of the items of the same manufacturer | customer rating realtive to the average rating of the items of the same category | . #collapse-show # price ratio items[&#39;recommended_simulation_price_ratio&#39;] = items[&#39;simulationPrice&#39;] / items[&#39;recommendedRetailPrice&#39;] # detailed item category items[&#39;category&#39;] = items[&#39;category1&#39;].astype(str) + items[&#39;category2&#39;].astype(str) + items[&#39;category3&#39;].astype(str) items[&#39;category&#39;] = items[&#39;category&#39;].astype(int) # customer rating ratio per manufacturer rating_manufacturer = items.groupby(&#39;manufacturer&#39;)[&#39;customerRating&#39;].agg(&#39;mean&#39;).reset_index() rating_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_customerRating_manufacturer&#39;] items = items.merge(rating_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) items[&#39;customerRating_manufacturer_ratio&#39;] = items[&#39;customerRating&#39;] / items[&#39;mean_customerRating_manufacturer&#39;] del items[&#39;mean_customerRating_manufacturer&#39;] # customer rating ratio per category rating_category = items.groupby(&#39;category&#39;)[&#39;customerRating&#39;].agg(&#39;mean&#39;).reset_index() rating_category.columns = [&#39;category&#39;, &#39;mean_customerRating_category&#39;] items = items.merge(rating_category, how = &#39;left&#39;, on = &#39;category&#39;) items[&#39;customerRating_category_ratio&#39;] = items[&#39;customerRating&#39;] / items[&#39;mean_customerRating_category&#39;] del items[&#39;mean_customerRating_category&#39;] . . We can now merge orders and items. We also partition the data into the labeled training set and the unlabeled test set, compute some missing features for the test set and export the data as csv. . #collapse-hide ########## DATA PARTITIONING # merge data df = pd.merge(orders, items, on = &#39;itemID&#39;, how = &#39;left&#39;) # partition intro train and test df_train = df[df[&#39;day_of_year&#39;] &lt; df[&#39;day_of_year&#39;].max()] df_test = df[df[&#39;day_of_year&#39;] == df[&#39;day_of_year&#39;].max()] ########## COMPUTE FEATURES FOR TEST DATA # add promotion info to test promo_vars = df_test.filter(like = &#39;promotion_&#39;).columns df_test[&#39;promo_in_test&#39;] = 3 - df_test[promo_vars].isnull().sum(axis = 1) df_test[&#39;promo_in_test&#39;].describe() del df_test[&#39;promo_in_test_manufacturer&#39;], df_test[&#39;promo_in_test_category&#39;] # future promo per manufacturer tmp_df_manufacturer = df_test.groupby(&#39;manufacturer&#39;)[&#39;promo_in_test&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;promo_in_test_manufacturer&#39;] df_test = df_test.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future promo per category tmp_df_category = df_test.groupby(&#39;category&#39;)[&#39;promo_in_test&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;promo_in_test_category&#39;] df_test = df_test.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) del df_test[&#39;mean_price_test_manufacturer&#39;], df_test[&#39;mean_price_test_category&#39;] # future price per manufacturer tmp_df_manufacturer = df_test.groupby(&#39;manufacturer&#39;)[&#39;mean_price_test&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_price_test_manufacturer&#39;] df_test = df_test.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future price per category tmp_df_category = df_test.groupby(&#39;category&#39;)[&#39;mean_price_test&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;mean_price_test_category&#39;] df_test = df_test.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # mean price ratios for var in price_vars: df_test[&#39;ratio_&#39; + str(var)] = df_test[&#39;mean_price_test&#39;] / df_test[var] df_test[&#39;ratio_manufacturer_&#39; + str(var)] = df_test[&#39;mean_price_test_manufacturer&#39;] / df_test[var] df_test[&#39;ratio_category_&#39; + str(var)] = df_test[&#39;mean_price_test_category&#39;] / df_test[var] ########## DROP FEATURES # drop promotion dates df_test.drop(promo_vars, axis = 1, inplace = True) df_train.drop(promo_vars, axis = 1, inplace = True) # drop mean prices price_vars = price_vars + [&#39;mean_price_test_manufacturer&#39;, &#39;mean_price_test_category&#39;] df_test.drop(price_vars, axis = 1, inplace = True) df_train.drop(price_vars, axis = 1, inplace = True) # export data save_csv_version(&#39;../data/prepared/df.csv&#39;, df_train, index = False, compression = &#39;gzip&#39;) save_csv_version(&#39;../data/prepared/df_test.csv&#39;, df_test, index = False, compression = &#39;gzip&#39;, min_version = 3) print(df_train.shape) print(df_test.shape) . . Saved as ../data/prepared/df_v14.csv Saved as ../data/prepared/df_test_v14.csv (1381116, 476) (10463, 476) . 4. Modeling . Custom loss functions . Mahcine learning encompasses a wide range of statically-inspired performance metrics such as MSE, MAE and others. In practice, machine learning models are used by a company that has specific goals. Usually, these goals can not be expressed in terms of such simple metrics. Therefore, it is important to come up with an evaluation metric consistent with the company&#39;s objectives to ensure that we judge performance on a criterion that actually matters. . In the DMC 2020 task, we are given a profit function of a retailer who is doing demand forecasting. The function accounts for asymmetric error costs: underpredicting demand results in lost revenue because the retailer can not sell a product that is not ready to ship, whereas overpredicting demand incurs a fee for storing the excessive amount of product. . Below, we derive profit according to the task description: . . Let&#39;s implement the profit function in Python: . #collapse-show def profit(y_true, y_pred, price): &#39;&#39;&#39; Computes profit according to DMC 2020 task. Arguments: - y_true (numpy array or list): ground truth (correct) target values. - y_pred (numpy array or list): estimated target values. - price (numpy array or list): item prices. Returns: - profit value &#39;&#39;&#39; # remove negative and round y_pred = np.where(y_pred &gt; 0, y_pred, 0) y_pred = np.round(y_pred).astype(&#39;int&#39;) # sold units units_sold = np.minimum(y_true, y_pred) # overstocked units units_overstock = y_pred - y_true units_overstock[units_overstock &lt; 0] = 0 # profit revenue = units_sold * price fee = units_overstock * price * 0.6 profit = revenue - fee profit = profit.sum() return profit . . The function above is great for evaluation. Can we go further and optimize it during modeling? . We will use LightGBM which supports custom loss functions on training and validation stages. In order to use a custom loss on the the training stage, one needs to define a function with its first and second-order derivatives. . A straightfoward approach would be to define the loss as a difference between the profit conditional on our demand prediction and oracle profit (when demand prediction is correct). However, such a loss would not be differentiable. This means that we could not compute derivatives to plug it as a training loss. Instead, we could come up with a slightly different function that approximates profit and satisfies the loss conditions. . We define the loss as a squared difference between the oracle profit and profit based on predicted demand. In this setting, we can compute loss derivatives with respect to the prediction (Gradient and Hessian): . . The snippet below implements the training and validation losses for LightGBM. You can notice that we do not include the squared prices in the loss functions. The reason is that with sklearn API, it is difficult to include external variables like prices in the loss. . # collpase-show ##### TRAINING LOSS def asymmetric_mse(y_true, y_pred): &#39;&#39;&#39; Asymmetric MSE objective for training LightGBM regressor. Arguments: - y_true (numpy array or list): ground truth (correct) target values. - y_pred (numpy array or list): estimated target values. Returns: - gradient - hessian &#39;&#39;&#39; residual = (y_true - y_pred).astype(&#39;float&#39;) grad = np.where(residual &gt; 0, -2*residual, -0.72*residual) hess = np.where(residual &gt; 0, 2.0, 0.72) return grad, hess ##### VALIDATION LOSS def asymmetric_mse_eval(y_true, y_pred): &#39;&#39;&#39; Asymmetric MSE evaluation metric for LightGBM regressor. Arguments: - y_true (numpy array or list): ground truth (correct) target values. - y_pred (numpy array or list): estimated target values. Returns: - metric name - metric value - whether the metric is maximized &#39;&#39;&#39; residual = (y_true - y_pred).astype(&#39;float&#39;) loss = np.where(residual &gt; 0, 2*residual**2, 0.72*residual**2) return &#39;asymmetric_mse_eval&#39;, np.mean(loss), False . So how to deal with item prices? . One option would be to account for prices within the fit() method. LightGBM supports weighting observations on both training and validation stages using the arguments sample_weight and eval_sample_weight. You will see how we supply price vectors in the modeling code in the next section. Note that including prices as weights instead of plugging them into the loss leads to losing some information, since Gradients and Hessians are computed without the price multiplication. Still, this approach provides a pretty close approximation of the original profit-driven loss. If you are interested in including prices in the loss, you can check lightGBM API that allows more flexibility. . The only missing piece is the relationship between the penalty size and the prediction error. By taking a square root of the profit differences instead of the absolute value, we penalize larger errors more than the smaller ones. However, our profit changes linearly with the error size. This is how we can can address it: . transform target using a non-linear transofmration (e.g. square root) | train a model that optimzes the MSE loss on the transformed target | apply the inverse transformation to the model predictions | . Target transformation smooths out the square effect in MSE. We still penalize large errors more, but the large errors on a transformed scale are also smaller compared to the original scale. This helps to balance the two effects and approximate a linear relationship between the error size and the loss penalty. . Modeling pipeline . Let&#39;s start building models! First, we extract the target and flag ID features not used for prediction. . #collapse-hide # extract target y = df_train[&#39;target&#39;] X = df_train.drop(&#39;target&#39;, axis = 1) del df_train print(X.shape, y.shape) # format test data X_test = df_test.drop(&#39;target&#39;, axis = 1) del df_test print(X_test.shape) # relevant features drop_feats = [&#39;itemID&#39;, &#39;day_of_year&#39;] features = [var for var in X.columns if var not in drop_feats] . . (1381116, 475) (1381116,) (10463, 475) . The modeling pipeline uses multiple tricks discovered during the model refinement process. We toogle these tricks using logical variables that define the following training options: . target_transform = True: transforms target to reduce penalty for large errors. Motivation for this is provided in the previous section. | train_on_positive = False: trains only on cases with positive sales (i.e., at least one of the order lags is greater than zero) and predicts null demand for items with no sales. This substantially reduces the training time but also leads to a drop in the performance. | two_stage = True: trains a two-stage model: (i) binary classifier predicting whether the future sales will be zero; (ii) regression model predicting the volume of sales. Predictions of the regression model are only stored for cases where the classifier predicts positive sales. | tuned_params = True: imports optimized LightGBM hyper-parameter values. The next section describes the tuning procedure. | . #collapse-hide ##### TRAINING OPTIONS # target transformation target_transform = True # train on positive sales only train_on_positive = False # two-stage model two_stage = True # use tuned meta-params tuned_params = True ##### CLASSIFIER PARAMETERS # rounds and options cores = 4 stop_rounds = 100 verbose = 100 seed = 23 # LGB parameters lgb_params = { &#39;boosting_type&#39;: &#39;goss&#39;, &#39;objective&#39;: asymmetric_mse, &#39;metrics&#39;: asymmetric_mse_eval, &#39;n_estimators&#39;: 1000, &#39;learning_rate&#39;: 0.1, &#39;bagging_fraction&#39;: 0.8, &#39;feature_fraction&#39;: 0.8, &#39;lambda_l1&#39;: 0.1, &#39;lambda_l2&#39;: 0.1, &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39; : cores, &#39;random_state&#39;: seed, } # load optimal parameters if tuned_params: par_file = open(&#39;../lgb_meta_params_100.pkl&#39;, &#39;rb&#39;) lgb_params = pickle.load(par_file) lgb_params[&#39;nthread&#39;] = cores lgb_params[&#39;random_state&#39;] = seed # second-stage LGB if two_stage: lgb_classifier_params = lgb_params.copy() lgb_classifier_params[&#39;objective&#39;] = &#39;binary&#39; lgb_classifier_params[&#39;metrics&#39;] = &#39;logloss&#39; . . We also define the partitioning parameters. We use a sliding window approach with 7 folds, where each subsequent fold is shifted by one day into the past. . #collapse-hide num_folds = 7 # no. CV folds test_days = 14 # no. days in the test set . . Let&#39;s explain the partitioning using the first fold as an example. Each fold is divided into training and validation subsets. The first 35 days are cut off and only used to compute lag-based features for the days starting from 36. Days 36 - 145 are used for training. For each of these days, we have features based on the previous 35 days and targets based on the next 14 days. Days 159 - 173 are used for validation. Days 146 - 158 between training and validation subsets are skipped to avoid data leakage: the target for these days would use information from the validation period. . . We can now set up a modeling loop with the following steps for each of the folds: . extract data from the fold and partition it into training and validation sets | train LightGBM on the training set and perform early stopping on the validation set | save predictions for the validation set (denoted as OOF) and predictions for the test set | save feature importance and performance on the validation fold | . #collapse-show # placeholders importances = pd.DataFrame() preds_oof = np.zeros((num_folds, items.shape[0])) reals_oof = np.zeros((num_folds, items.shape[0])) prices_oof = np.zeros((num_folds, items.shape[0])) preds_test = np.zeros(items.shape[0]) oof_rmse = [] oof_profit = [] oracle_profit = [] clfs = [] train_idx = [] valid_idx = [] # objects train_days = X[&#39;day_of_year&#39;].max() - test_days + 1 - num_folds - X[&#39;day_of_year&#39;].min() # no. days in the train set time_start = time.time() # modeling loop for fold in range(num_folds): ##### PARTITIONING # dates if fold == 0: v_end = X[&#39;day_of_year&#39;].max() else: v_end = v_end - 1 v_start = v_end t_end = v_start - (test_days + 1) t_start = t_end - (train_days - 1) # extract index train_idx.append(list(X[(X.day_of_year &gt;= t_start) &amp; (X.day_of_year &lt;= t_end)].index)) valid_idx.append(list(X[(X.day_of_year &gt;= v_start) &amp; (X.day_of_year &lt;= v_end)].index)) # extract samples X_train, y_train = X.iloc[train_idx[fold]][features], y.iloc[train_idx[fold]] X_valid, y_valid = X.iloc[valid_idx[fold]][features], y.iloc[valid_idx[fold]] X_test = X_test[features] # keep positive cases if train_on_positive: y_train = y_train.loc[(X_train[&#39;order_sum_last_28&#39;] &gt; 0) | (X_train[&#39;promo_in_test&#39;] &gt; 0)] X_train = X_train.loc[(X_train[&#39;order_sum_last_28&#39;] &gt; 0) | (X_train[&#39;promo_in_test&#39;] &gt; 0)] # information print(&#39;-&#39; * 65) print(&#39;- train period days: {} -- {} (n = {})&#39;.format(t_start, t_end, len(train_idx[fold]))) print(&#39;- valid period days: {} -- {} (n = {})&#39;.format(v_start, v_end, len(valid_idx[fold]))) print(&#39;-&#39; * 65) ##### MODELING # target transformation if target_transform: y_train = np.sqrt(y_train) y_valid = np.sqrt(y_valid) # first stage model if two_stage: y_train_binary, y_valid_binary = y_train.copy(), y_valid.copy() y_train_binary[y_train_binary &gt; 0] = 1 y_valid_binary[y_valid_binary &gt; 0] = 1 clf_classifier = lgb.LGBMClassifier(**lgb_classifier_params) clf_classifier = clf_classifier.fit(X_train, y_train_binary, eval_set = [(X_train, y_train_binary), (X_valid, y_valid_binary)], eval_metric = &#39;logloss&#39;, early_stopping_rounds = stop_rounds, verbose = verbose) preds_oof_fold_binary = clf_classifier.predict(X_valid) preds_test_fold_binary = clf_classifier.predict(X_test) # training clf = lgb.LGBMRegressor(**lgb_params) clf = clf.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_valid, y_valid)], eval_metric = asymmetric_mse_eval, sample_weight = X_train[&#39;simulationPrice&#39;].values, eval_sample_weight = [X_train[&#39;simulationPrice&#39;].values, X_valid[&#39;simulationPrice&#39;].values], early_stopping_rounds = stop_rounds, verbose = verbose) clfs.append(clf) # inference if target_transform: preds_oof_fold = postprocess_preds(clf.predict(X_valid)**2) reals_oof_fold = y_valid**2 preds_test_fold = postprocess_preds(clf.predict(X_test)**2) / num_folds else: preds_oof_fold = postprocess_preds(clf.predict(X_valid)) reals_oof_fold = y_valid preds_test_fold = postprocess_preds(clf.predict(X_test)) / num_folds # impute zeros if train_on_positive: preds_oof_fold[(X_valid[&#39;order_sum_last_28&#39;] == 0) &amp; (X_valid[&#39;promo_in_test&#39;] == 0)] = 0 preds_test_fold[(X_test[&#39;order_sum_last_28&#39;] == 0) &amp; (X_test[&#39;promo_in_test&#39;] == 0)] = 0 # multiply with first stage predictions if two_stage: preds_oof_fold = preds_oof_fold * np.round(preds_oof_fold_binary) preds_test_fold = preds_test_fold * np.round(preds_test_fold_binary) # write predictions preds_oof[fold, :] = preds_oof_fold reals_oof[fold, :] = reals_oof_fold preds_test += preds_test_fold # save prices prices_oof[fold, :] = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values ##### EVALUATION # evaluation oof_rmse.append(np.sqrt(mean_squared_error(reals_oof[fold, :], preds_oof[fold, :]))) oof_profit.append(profit(reals_oof[fold, :], preds_oof[fold, :], price = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values)) oracle_profit.append(profit(reals_oof[fold, :], reals_oof[fold, :], price = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values)) # feature importance fold_importance_df = pd.DataFrame() fold_importance_df[&#39;Feature&#39;] = features fold_importance_df[&#39;Importance&#39;] = clf.feature_importances_ fold_importance_df[&#39;Fold&#39;] = fold + 1 importances = pd.concat([importances, fold_importance_df], axis = 0) # information print(&#39;-&#39; * 65) print(&#39;FOLD {:d}/{:d}: RMSE = {:.2f}, PROFIT = {:.0f}&#39;.format(fold + 1, num_folds, oof_rmse[fold], oof_profit[fold])) print(&#39;-&#39; * 65) print(&#39;&#39;) # print performance print(&#39;&#39;) print(&#39;-&#39; * 65) print(&#39;- AVERAGE RMSE: {:.2f}&#39;.format(np.mean(oof_rmse))) print(&#39;- AVERAGE PROFIT: {:.0f} ({:.2f}%)&#39;.format(np.mean(oof_profit), 100 * np.mean(oof_profit) / np.mean(oracle_profit))) print(&#39;- RUNNING TIME: {:.2f} minutes&#39;.format((time.time() - time_start) / 60)) print(&#39;-&#39; * 65) . . -- - train period days: 41 -- 151 (n = 1161393) - valid period days: 166 -- 166 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [532] training&#39;s binary_logloss: 0.238417 valid_1&#39;s binary_logloss: 0.347182 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [788] training&#39;s rmse: 0.611924 training&#39;s asymmetric_mse_eval: 2.82734 valid_1&#39;s rmse: 0.98004 valid_1&#39;s asymmetric_mse_eval: 5.83945 -- FOLD 1/7: RMSE = 74.46, PROFIT = 4146664 -- -- - train period days: 40 -- 150 (n = 1161393) - valid period days: 165 -- 165 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [1022] training&#39;s binary_logloss: 0.216471 valid_1&#39;s binary_logloss: 0.345619 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [901] training&#39;s rmse: 0.599668 training&#39;s asymmetric_mse_eval: 2.75637 valid_1&#39;s rmse: 0.94857 valid_1&#39;s asymmetric_mse_eval: 5.49861 -- FOLD 2/7: RMSE = 71.49, PROFIT = 4085941 -- -- - train period days: 39 -- 149 (n = 1161393) - valid period days: 164 -- 164 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [643] training&#39;s binary_logloss: 0.230494 valid_1&#39;s binary_logloss: 0.347911 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [699] training&#39;s rmse: 0.620079 training&#39;s asymmetric_mse_eval: 2.87369 valid_1&#39;s rmse: 0.954063 valid_1&#39;s asymmetric_mse_eval: 5.44372 -- FOLD 3/7: RMSE = 69.23, PROFIT = 4100618 -- -- - train period days: 38 -- 148 (n = 1161393) - valid period days: 163 -- 163 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [676] training&#39;s binary_logloss: 0.227568 valid_1&#39;s binary_logloss: 0.351372 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [547] training&#39;s rmse: 0.645373 training&#39;s asymmetric_mse_eval: 3.09249 valid_1&#39;s rmse: 0.958953 valid_1&#39;s asymmetric_mse_eval: 5.33328 -- FOLD 4/7: RMSE = 68.55, PROFIT = 4009929 -- -- - train period days: 37 -- 147 (n = 1161393) - valid period days: 162 -- 162 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [552] training&#39;s binary_logloss: 0.231805 valid_1&#39;s binary_logloss: 0.354317 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [788] training&#39;s rmse: 0.603891 training&#39;s asymmetric_mse_eval: 2.76021 valid_1&#39;s rmse: 0.943618 valid_1&#39;s asymmetric_mse_eval: 5.01711 -- FOLD 5/7: RMSE = 62.10, PROFIT = 3946376 -- -- - train period days: 36 -- 146 (n = 1161393) - valid period days: 161 -- 161 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [711] training&#39;s binary_logloss: 0.223124 valid_1&#39;s binary_logloss: 0.349099 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [593] training&#39;s rmse: 0.631258 training&#39;s asymmetric_mse_eval: 2.96402 valid_1&#39;s rmse: 0.957476 valid_1&#39;s asymmetric_mse_eval: 5.36804 -- FOLD 6/7: RMSE = 67.31, PROFIT = 3735558 -- -- - train period days: 35 -- 145 (n = 1161393) - valid period days: 160 -- 160 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [711] training&#39;s binary_logloss: 0.22193 valid_1&#39;s binary_logloss: 0.338637 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [782] training&#39;s rmse: 0.600011 training&#39;s asymmetric_mse_eval: 2.71385 valid_1&#39;s rmse: 0.987073 valid_1&#39;s asymmetric_mse_eval: 5.67223 -- FOLD 7/7: RMSE = 74.46, PROFIT = 3958098 -- -- - AVERAGE RMSE: 69.66 - AVERAGE PROFIT: 3997598 (54.19%) - RUNNING TIME: 88.53 minutes -- . Looks good! The modeling pipeline took us about 1.5 hours to run. . Forecasting demand with our models results in 3,997,598 daily profit, which is about 54% of the maximum possible profit. Let&#39;s visualize the results: . # collapse-hide fig = plt.figure(figsize = (20, 7)) # residual plot plt.subplot(1, 2, 1) plt.scatter(reals_oof.reshape(-1), preds_oof.reshape(-1)) axis_lim = np.max([reals_oof.max(), preds_oof.max()]) plt.ylim(top = 1.02*axis_lim) plt.xlim(right = 1.02*axis_lim) plt.plot((0, axis_lim), (0, axis_lim), &#39;r--&#39;) plt.title(&#39;Residual Plot&#39;) plt.ylabel(&#39;Predicted demand&#39;) plt.xlabel(&#39;Actual demand&#39;) # feature importance plt.subplot(1, 2, 2) top_feats = 50 cols = importances[[&#39;Feature&#39;, &#39;Importance&#39;]].groupby(&#39;Feature&#39;).mean().sort_values(by = &#39;Importance&#39;, ascending = False)[0:top_feats].index importance = importances.loc[importances.Feature.isin(cols)] sns.barplot(x = &#39;Importance&#39;, y = &#39;Feature&#39;, data = importance.sort_values(by = &#39;Importance&#39;, ascending = False), ci = 0) plt.title(&#39;Feature Importance&#39;) plt.tight_layout() . . . The scatterplot shows that there is a space for further improvement: many predictions are far from the 45-degree line where predicted and real orders are equal. The important features mostly contain price information followed by features that count the previous orders. . We can now use predictions stored in preds_test to create a submission. Mission acomplished! . 5. Hyper-parameter tuning . One way to improve our solution is to optimize the LightGBM hyper-parameters. . We tune hyper-parameters using the hyperopt package, which performs optimization using Tree of Parzen Estimators (TPE) as a search algorithm. You don&#39;t really need to know how TPE works. As a user, you are only required to supply a parameter grid indicating the range of possible values. Compared to standard tuning methods like grid search or random search, TPE explores the search space more efficiently, allowing you to find a suitable solution faster. If you want to read more, see the package documentation here. . So, let&#39;s specify hyper-parameter ranges! We create a dictionary using the following options: . hp.choice(&#39;name&#39;, list_of_values): sets a hyper-parameter to one of the values from a list. This is suitable for hyper-parameters that can have multiple distinct values like boosting_type | hp.uniform(&#39;name&#39;, min, max): sets a hyper-parameter to a float between min and max. This works well with hyper-parameters such as learning_rate | hp.quniform(&#39;name&#39;, min, max, step): sets a hyper-parameter to a value between min and max with a step size of step. This is useful for integer parameters like max_depth | . #collapse-show # training params lgb_reg_params = { &#39;boosting_type&#39;: hp.choice(&#39;boosting_type&#39;, [&#39;gbdt&#39;, &#39;goss&#39;]), &#39;objective&#39;: &#39;rmse&#39;, &#39;metrics&#39;: &#39;rmse&#39;, &#39;n_estimators&#39;: 10000, &#39;learning_rate&#39;: hp.uniform(&#39;learning_rate&#39;, 0.0001, 0.3), &#39;max_depth&#39;: hp.quniform(&#39;max_depth&#39;, 1, 16, 1), &#39;num_leaves&#39;: hp.quniform(&#39;num_leaves&#39;, 10, 64, 1), &#39;bagging_fraction&#39;: hp.uniform(&#39;bagging_fraction&#39;, 0.3, 1), &#39;feature_fraction&#39;: hp.uniform(&#39;feature_fraction&#39;, 0.3, 1), &#39;lambda_l1&#39;: hp.uniform(&#39;lambda_l1&#39;, 0, 1), &#39;lambda_l2&#39;: hp.uniform(&#39;lambda_l2&#39;, 0, 1), &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39; : 4, &#39;random_state&#39;: 77, } # evaluation params lgb_fit_params = { &#39;eval_metric&#39;: &#39;rmse&#39;, &#39;early_stopping_rounds&#39;: 100, &#39;verbose&#39;: False, } # combine params lgb_space = dict() lgb_space[&#39;reg_params&#39;] = lgb_reg_params lgb_space[&#39;fit_params&#39;] = lgb_fit_params . . Next, we create HPOpt object that performs tuning. We can avoid this in a simple tuning task, but defining an object gives us more control of the optimization process, which is useful with a custom loss. We define three object methods: . process: runs optimization. By default, HPO uses fmin() to minimize the specified loss | lgb_reg: initializes LightGBM model | train_reg: trains LightGBM and computes the loss. Since we aim to maximize profit, we simply define loss as negative profit | . # collapse-show class HPOpt(object): # INIT def __init__(self, x_train, x_test, y_train, y_test): self.x_train = x_train self.x_test = x_test self.y_train = y_train self.y_test = y_test # optimization process def process(self, fn_name, space, trials, algo, max_evals): fn = getattr(self, fn_name) try: result = fmin(fn = fn, space = space, algo = algo, max_evals = max_evals, trials = trials) except Exception as e: return {&#39;status&#39;: STATUS_FAIL, &#39;exception&#39;: str(e)} return result, trials # LGBM initialization def lgb_reg(self, para): para[&#39;reg_params&#39;][&#39;max_depth&#39;] = int(para[&#39;reg_params&#39;][&#39;max_depth&#39;]) para[&#39;reg_params&#39;][&#39;num_leaves&#39;] = int(para[&#39;reg_params&#39;][&#39;num_leaves&#39;]) reg = lgb.LGBMRegressor(**para[&#39;reg_params&#39;]) return self.train_reg(reg, para) # training and inference def train_reg(self, reg, para): # fit LGBM reg.fit(self.x_train, self.y_train, eval_set = [(self.x_train, self.y_train), (self.x_test, self.y_test)], sample_weight = self.x_train[&#39;simulationPrice&#39;].values, eval_sample_weight = [self.x_train[&#39;simulationPrice&#39;].values, self.x_test[&#39;simulationPrice&#39;].values], **para[&#39;fit_params&#39;]) # inference if target_transform: preds = postprocess_preds(reg.predict(self.x_test)**2) reals = self.y_test**2 else: preds = postprocess_preds(reg.predict(self.x_test)) reals = self.y_test # compute loss [negative profit] loss = np.round(-profit(reals, preds, price = self.x_test[&#39;simulationPrice&#39;].values)) return {&#39;loss&#39;: loss, &#39;status&#39;: STATUS_OK} . . To prevent overfitting, we perform tuning on a differet subset of data compared to the models trained in the previous section by going one day further in the past. . # collapse-hide # validation dates v_end = 158 # 1 day before last validation fold in code_03_modeling v_start = v_end # same as v_start # training dates t_start = 28 # first day in the data t_end = v_start - 15 # validation day - two weeks # extract index train_idx = list(X[(X.day_of_year &gt;= t_start) &amp; (X.day_of_year &lt;= t_end)].index) valid_idx = list(X[(X.day_of_year &gt;= v_start) &amp; (X.day_of_year &lt;= v_end)].index) # extract samples X_train, y_train = X.iloc[train_idx][features], y.iloc[train_idx] X_valid, y_valid = X.iloc[valid_idx][features], y.iloc[valid_idx] # target transformation if target_transform: y_train = np.sqrt(y_train) y_valid = np.sqrt(y_valid) # information print(&#39;-&#39; * 65) print(&#39;- train period days: {} -- {} (n = {})&#39;.format(t_start, t_end, len(train_idx))) print(&#39;- valid period days: {} -- {} (n = {})&#39;.format(v_start, v_end, len(valid_idx))) print(&#39;-&#39; * 65) . . -- - train period days: 28 -- 143 (n = 1213708) - valid period days: 158 -- 158 (n = 10463) -- . Now, we just need to instantiate the HPOpt object and launch the tuning trials! The optimization will run automatically, and we would only need to extract the optimized values: . # collapse-show # instantiate objects hpo_obj = HPOpt(X_train, X_valid, y_train, y_valid) trials = Trials() # perform tuning lgb_opt_params = hpo_obj.process(fn_name = &#39;lgb_reg&#39;, space = lgb_space, trials = trials, algo = tpe.suggest, max_evals = tuning_trials) # merge best params to fixed params params = list(lgb_opt_params[0].keys()) for par_id in range(len(params)): lgb_reg_params[params[par_id]] = lgb_opt_params[0][params[par_id]] # postprocess lgb_reg_params[&#39;boosting_type&#39;] = boost_types[lgb_reg_params[&#39;boosting_type&#39;]] lgb_reg_params[&#39;max_depth&#39;] = int(lgb_reg_params[&#39;max_depth&#39;]) lgb_reg_params[&#39;num_leaves&#39;] = int(lgb_reg_params[&#39;num_leaves&#39;]) # print best params print(&#39;Best meta-parameters:&#39;) lgb_reg_params . . Best meta-parameters: . {&#39;boosting_type&#39;: &#39;goss&#39;, &#39;objective&#39;: &#39;rmse&#39;, &#39;metrics&#39;: &#39;rmse&#39;, &#39;n_estimators&#39;: 10000, &#39;learning_rate&#39;: 0.004012417857266637, &#39;max_depth&#39;: 10, &#39;num_leaves&#39;: 64, &#39;bagging_fraction&#39;: 0.9346881591116736, &#39;feature_fraction&#39;: 0.6680768850934483, &#39;lambda_l1&#39;: 0.28013320828944976, &#39;lambda_l2&#39;: 0.5896826524767101, &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39;: 4, &#39;random_state&#39;: 77} . Done! Now we can save the optimized values and import them when setting up the model. . # collapse-hide par_file = open(&#39;../lgb_meta_params.pkl&#39;, &#39;wb&#39;) pickle.dump(lgb_reg_params, par_file) par_file.close() . . 6. Closing words . This blogpost has finally come to an end. Thank you for reading! . We looked at important stages of our solution and covered steps such as data aggregation, feature engineering, custom loss functions, target transformation and hyper-parameter tuning. . Our final solution was a simple ensemble of multiple LightGBM models with different features and training options discussed in this post. If you are interested in the ensembling part, you can find the codes in my Github. . Please feel free to use the comment widnow below to ask questions and stay tuned for the next editions of Data Mining Cup! .",
            "url": "https://kozodoi.me/python/time%20series/demand%20forecasting/competitions/2020/07/27/demand-forecasting.html",
            "relUrl": "/python/time%20series/demand%20forecasting/competitions/2020/07/27/demand-forecasting.html",
            "date": " â€¢ Jul 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Detecting Blindness with Deep Learning",
            "content": "1. Overview . Can deep learning help to detect blindness? . This blogpost describes a project that develops a convolutional neural network (CNN) for predicting the severity of the diabetic retinopathy based on the patient&#39;s retina photos. The project was completed within the scope of the Udacity ML Engineer nano-degree program and the Kaggle competition hosted by the Asia Pacific Tele-Ophthalmology Society (APTOS). . The blogpost provides a project walkthrough covering the following steps: . data exploration and image preprocessing to normalize images from different clinics | using transfer learning to pre-train CNN on a larger data set | employing techniques such as learning rate scheduler, test-time augmentation and others | . The modeling is performed in PyTorch. All notebooks and a PDF report are available on Github. . 2. Motivation . Diabetic retinopathy (DR) is one of the leading causes of vision loss. The World Health Organization reports that more than 300 million people worldwide have diabetes (Wong et al. 2016). In 2019, the global prevalence of DR among individuals with diabetes was at more than 25% (Thomas et al. 2019). The prevalence has been rising rapidly in developing countries. . Early detection and treatment are crucial steps towards preventing DR. The screening procedure requires a trained clinical expert to examine the fundus photographs of the patient&#39;s retina. This creates delays in diagnosis and treatment. This is especially relevant for developing countries, which often lack qualified medical staff to perform the diagnosis. Automated detection of DR can speed up the efficiency and coverage of the screening programs. . . Image source: https://www.eyeops.com/contents/our-services/eye-diseases/diabetic-retinopathy . 3. Data preparation . Data preparation is a very important step that is frequently underestimated. The quality of the input data has a strong impact on the resulting performance of the developed machine learning models. Therefore, it is crucial to take some time to look at the data and think about possible issues that should be addressed before moving on to the modeling stage. Let&#39;s do that! . Data exploration . The data set is available for the download at the competition&#39;s website. The data includes 3,662 labeled retina images of clinical patients and a test set with 1,928 images with unknown labels. . The images are labeled by a clinical expert. The integer labels indicate the severity of DR on a scale from 0 to 4, where 0 indicates no disease and 5 is the proliferative stage of DR. . Let&#39;s start by importing the data and looking at the class distribution. . #collapse-hide ############ PACKAGES import numpy as np import pandas as pd import torch import torchvision from torchvision import transforms, datasets from torch.utils.data import Dataset from PIL import Image, ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True import cv2 from tqdm import tqdm_notebook as tqdm import random import time import sys import os import math import matplotlib.pyplot as plt import seaborn as sns ########## SETTINGS pd.set_option(&#39;display.max_columns&#39;, None) %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) ############ CLASS DISTRIBUTION # import data train = pd.read_csv(&#39;../input/aptos2019-blindness-detection/train.csv&#39;) test = pd.read_csv(&#39;../input/aptos2019-blindness-detection/sample_submission.csv&#39;) # plot fig = plt.figure(figsize = (15, 5)) plt.hist(train[&#39;diagnosis&#39;]) plt.title(&#39;Class Distribution&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Diagnosis&#39;) . . The data is imbalanced: 49% images are from healthy patients. The remaining 51% are different stages of DR. The least common class is 3 (severe stage) with only 5% of the total examples. . The data is collected from multiple clinics using a variety of camera models, which creates discrepancies in the image resolution, aspect ratio and other parameters. This is demonstrated in the snippet below, where we plot histograms of image width, height and aspect ratio. . #collapse-hide # placeholder image_stats = [] # import loop for index, observation in tqdm(train.iterrows(), total = len(train)): # import image img = cv2.imread(&#39;../input/aptos2019-blindness-detection/train_images/{}.png&#39;.format(observation[&#39;id_code&#39;])) # compute stats height, width, channels = img.shape ratio = width / height # save image_stats.append(np.array((observation[&#39;diagnosis&#39;], height, width, channels, ratio))) # construct DF image_stats = pd.DataFrame(image_stats) image_stats.columns = [&#39;diagnosis&#39;, &#39;height&#39;, &#39;width&#39;, &#39;channels&#39;, &#39;ratio&#39;] # create plot fig = plt.figure(figsize = (15, 5)) # width plt.subplot(1, 3, 1) plt.hist(image_stats[&#39;width&#39;]) plt.title(&#39;(a) Image Width&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Width&#39;) # height plt.subplot(1, 3, 2) plt.hist(image_stats[&#39;height&#39;]) plt.title(&#39;(b) Image Height&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Height&#39;) # ratio plt.subplot(1, 3, 3) plt.hist(image_stats[&#39;ratio&#39;]) plt.title(&#39;(c) Aspect Ratio&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Ratio&#39;) . . Now, let&#39;s look into the actual eyes! The code below creates the EyeData dataset class to import images. We also create a DataLoader object to load sample images and visualize the first batch. . #collapse-hide ##### DATASET # image preprocessing def prepare_image(path, image_size = 256): # import image = cv2.imread(path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # resize image = cv2.resize(image, (int(image_size), int(image_size))) # convert to tensor image = torch.tensor(image) image = image.permute(2, 1, 0) return image # dataset class EyeData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} ##### EXAMINE SAMPLE BATCH # transformations sample_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # dataset sample = EyeData(data = train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = sample_trans) # data loader sample_loader = torch.utils.data.DataLoader(dataset = sample, batch_size = 10, shuffle = False, num_workers = 4) # display images for batch_i, data in enumerate(sample_loader): # extract data inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1, 1) # create plot fig = plt.figure(figsize = (15, 7)) for i in range(len(labels)): ax = fig.add_subplot(2, len(labels)/2, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[i]) break . . . The illustration further emphasizes differences in the aspect ratio and lighting conditions. . The severity of DR is diagnosed by the presence of visual cues such as abnormal blood vessels, hard exudates and so-called cotton wool spots. You can read more about the diagnosing process here. Comparing the sample images, we can see the presence of exudates and cotton wool spots on some of the retina images of sick patients. . Image preprocessing . To simplify the classification task for our model, we need to ensure that retina images look similar. . First, using cameras with different aspect ratios results in some images having large black areas around the eye. The black areas do not contain information relevant for prediction and can be cropped. However, the size of black areas varies from one image to another. To address this, we develop a cropping function that converts the image to grayscale and marks black areas based on the pixel intensity. Next, we find a mask of the image by selecting rows and columns in which all pixels exceed the intensity threshold. This helps to remove vertical or horizontal rectangles filled with black similar to the ones observed in the upper-right image. After removing the black stripes, we resize the images to the same height and width. . Another issue is the eye shape. Depending on the image parameters, some eyes have a circular form, whereas others look like ovals. Since the size and shape of cues located in the retina determine the disease severity, it is crucial to standardize the eye shape as well. To do so, we develop another function that makes a circular crop around the center of the image. . Finally, we correct for the lightning and brightness discrepancies by smoothing the images using a Gaussian filter. . The snippet below provides the updated prepare_image() function that incorporates the discussed preprocessing steps. . #collapse-show ##### image preprocessing function def prepare_image(path, sigmaX = 10, do_random_crop = False): # import image image = cv2.imread(path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # perform smart crops image = crop_black(image, tol = 7) if do_random_crop == True: image = random_crop(image, size = (0.9, 1)) # resize and color image = cv2.resize(image, (int(image_size), int(image_size))) image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), sigmaX), -4, 128) # circular crop image = circle_crop(image, sigmaX = sigmaX) # convert to tensor image = torch.tensor(image) image = image.permute(2, 1, 0) return image ##### automatic crop of black areas def crop_black(img, tol = 7): if img.ndim == 2: mask = img &gt; tol return img[np.ix_(mask.any(1),mask.any(0))] elif img.ndim == 3: gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) mask = gray_img &gt; tol check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0] if (check_shape == 0): return img else: img1 = img[:,:,0][np.ix_(mask.any(1),mask.any(0))] img2 = img[:,:,1][np.ix_(mask.any(1),mask.any(0))] img3 = img[:,:,2][np.ix_(mask.any(1),mask.any(0))] img = np.stack([img1, img2, img3], axis = -1) return img ##### circular crop around image center def circle_crop(img, sigmaX = 10): height, width, depth = img.shape largest_side = np.max((height, width)) img = cv2.resize(img, (largest_side, largest_side)) height, width, depth = img.shape x = int(width / 2) y = int(height / 2) r = np.amin((x,y)) circle_img = np.zeros((height, width), np.uint8) cv2.circle(circle_img, (x,y), int(r), 1, thickness = -1) img = cv2.bitwise_and(img, img, mask = circle_img) return img ##### random crop def random_crop(img, size = (0.9, 1)): height, width, depth = img.shape cut = 1 - random.uniform(size[0], size[1]) i = random.randint(0, int(cut * height)) j = random.randint(0, int(cut * width)) h = i + int((1 - cut) * height) w = j + int((1 - cut) * width) img = img[i:h, j:w, :] return img . . Next, we define a new EyeData class that uses the new processing functions and visualize a batch of sample images after corrections. . #collapse-show ##### DATASET # dataset class class EyeData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} ##### EXAMINE SAMPLE BATCH image_size = 256 # transformations sample_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # dataset sample = EyeData(data = train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = sample_trans) # data loader sample_loader = torch.utils.data.DataLoader(dataset = sample, batch_size = 10, shuffle = False, num_workers = 4) # display images for batch_i, data in enumerate(sample_loader): # extract data inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1, 1) # create plot fig = plt.figure(figsize = (15, 7)) for i in range(len(labels)): ax = fig.add_subplot(2, len(labels)/2, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[i]) break . . . This looks much better! Comparing the retina images to the ones before the preprocessing, we can see that the apparent discrepancies between the photos are now fixed. The eyes now have a similar circular shape, and the color scheme is more consistent. This should help the model to detect the signs of the DR. . Check out this notebook by Nakhon Ratchasima for more ideas on the image preprocessing for retina photos. The functions in this project are largely inspired by his work during the competition. . 4. Modeling . CNNs achieve state-of-the-art performance in computer vision tasks. Recent medical research also shows a high potential of CNNs in DR classification (Gulshan et al. 2016). . In this project, we employ a CNN model with the EfficientNet architecture. EfficientNet is one of the recent state-of-the-art image classification models (Tan et al. 2019). It encompasses 8 architecture variants (B0 to B7) that differ in the model complexity and default image size. . The architecture of EfficientNet B0 is visualized below. We test multiple EfficientNet architectures and use the one that demonstrates the best performance. . . The modeling pipeline consists of three stages: . Pre-training. The data set has a limited number of images (N = 3,662). We pre-train the CNN model on a larger data set from the previous Kaggle competition. | Fine-tuning. We fine-tune the model on the target data set. We use cross-validation and make modeling decisions based on the performance of the out-of-fold predictions. | Inference. We aggregate predictions of the models trained on different combinations of training folds and use test-time augmentation to further improve the performance. | . Pre-training . Due to small sample size, we can not train a complex neural architecture from scratch. This is where transfer learning comes in handy. The idea of transfer learning is to pre-train a model on a different data (source domain) and fine-tune it on a relevant data set (target domain). . A good candidate for the source domain is the ImageNet database. Most published CNN models are trained on that data. However, ImageNet images are substantially different from the retina images we want to classify. Although initializing CNN with ImageNet weights might help the network to transfer the knowledge of basic image patterns such as shapes and edges, we still need to learn a lot from the target domain. . It turns out that APTOS had hosted another Kaggle competition on the DR classification in 2015. The data set of the 2015 competition features 35,126 retina images labeled by a clinician using the same scale as the target data set. The data is available for the download here. . This enables us to use following pipeline: . initialize weights from a CNN trained on ImageNet | train the CNN on the 2015 data set | fine-tune the CNN on the 2019 data set | . Let&#39;s start modeling! First, we enable GPU support and fix random seeds. The function seed_everything() sets seed for multiple packages, including numpy and pytorch, to ensure reproducibility. . #collapse-show # GPU check train_on_gpu = torch.cuda.is_available() if not train_on_gpu: print(&#39;CUDA is not available. Training on CPU...&#39;) device = torch.device(&#39;cpu&#39;) else: print(&#39;CUDA is available. Training on GPU...&#39;) device = torch.device(&#39;cuda:0&#39;) # seed function def seed_everything(seed = 23): os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # set seed seed = 23 seed_everything(seed) . . CUDA is available. Training on GPU... . Let&#39;s take a quick look at the class distribution in the 2015 data. . #collapse-show # import data train = pd.read_csv(&#39;../input/diabetic-retinopathy-resized/trainLabels.csv&#39;) train.columns = [&#39;id_code&#39;, &#39;diagnosis&#39;] test = pd.read_csv(&#39;../input/aptos2019-blindness-detection/train.csv&#39;) # check shape print(train.shape, test.shape) print(&#39;-&#39; * 15) print(train[&#39;diagnosis&#39;].value_counts(normalize = True)) print(&#39;-&#39; * 15) print(test[&#39;diagnosis&#39;].value_counts(normalize = True)) . . (35126, 2) (3662, 2) 0 0.734783 2 0.150658 1 0.069550 3 0.024853 4 0.020156 Name: diagnosis, dtype: float64 0 0.492900 2 0.272802 1 0.101038 4 0.080557 3 0.052703 Name: diagnosis, dtype: float64 . The imbalance in the source data is stronger than in the target data: 73% of images represent healthy patients, whereas the most severe stage of the DR is only found in 2% of the images. To address the imbalance, we will use the target data set as a validation sample during training. . We create two Dataset objects to enable different augmentations on training and inference stages: EyeTrainData and EyeTestData. The former includes a random crop that is skipped for the test data. . #collapse-hide # dataset class: train class EyeTrainData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.jpeg&#39;) image = prepare_image(img_name, do_random_crop = True) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} # dataset class: test class EyeTestData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name, do_random_crop = False) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} . . We use a batch size of 20 and set the image size of 256. The choice of these parameters is a trade-off between performance and resource capacity. Feel free to try larger image and batch sizes if you have resources. . We use the following data augmentations during training: . random horizontal flip | random vertical flip | random rotation in the range [-360 degrees, 360 degrees] | . #collapse-show # parameters batch_size = 20 image_size = 256 # train transformations train_trans = transforms.Compose([transforms.ToPILImage(), transforms.RandomRotation((-360, 360)), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor() ]) # validation transformations valid_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # create datasets train_dataset = EyeTrainData(data = train, directory = &#39;../input/diabetic-retinopathy-resized/resized_train/resized_train&#39;, transform = train_trans) valid_dataset = EyeTestData(data = test, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = valid_trans) # create data loaders train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4) valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) . . Next, let&#39;s instantiate the EfficentNet model. We use B4 architecture and initialize pre-trained ImageNet weights by downloading the model parameters in the PyTorch format. The convolutional part of the network responsible for feature extraction outputs a tensor with 1792 features. To adapt the CNN to our task, we replace the last fully-connected classification layer with a (1792, 5) fully-connected layer. . The CNN is instantiated with init_model(). The argument train ensures that we load ImageNet weights on the training stage and turn off gradient computation on the inference stage. . #collapse-show # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_name(&#39;efficientnet-b4&#39;) model._fc = nn.Linear(model._fc.in_features, 5) # freeze layers for param in model.parameters(): param.requires_grad = False return model # check architecture model = init_model() print(model) . . Since we are dealing with a multiple classification problem, we use cross-entropy as a loss function. We use nn.CrossEntropyLoss() which combines logsoftmax and negative log-likelihood loss and applies them to the output of the last network layer. . The Kaggle competition uses Cohen&#39;s kappa for evaluation. Kappa measures the agreement between the actual and predicted labels. Since Kappa is non-differentiable, we can not use it as a loss function. At the same time, we can use Kappa to evaluate the performance and early stop the training epochs. . We use Adam optimizer with a starting learning rate of 0.001. During training, we use a learning rate scheduler, which multiplies the learning rate by 0.5 after every 5 epochs. This helps to make smaller changes to the network weights when we are getting closer to the optimum. . #collapse-show # loss function criterion = nn.CrossEntropyLoss() # epochs max_epochs = 15 early_stop = 5 # learning rates eta = 1e-3 # scheduler step = 5 gamma = 0.5 # optimizer optimizer = optim.Adam(model.parameters(), lr = eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) # initialize model and send to GPU model = init_model() model = model.to(device) . . After each training epoch, we validate the model on the target data. We extract class scores from the last fully-connected layer and predict the image class corresponding to the highest score. We train the network for 15 epochs, tracking the validation loss and Cohen&#39;s kappa. If the kappa does not increase for 5 consecutive epochs, we terminate the training process and save model weights for the epoch associated with the highest validation kappa. . #collapse-show # placeholders oof_preds = np.zeros((len(test), 5)) val_kappas = [] val_losses = [] trn_losses = [] bad_epochs = 0 # timer cv_start = time.time() # training and validation loop for epoch in range(max_epochs): ##### PREPARATION # timer epoch_start = time.time() # reset losses trn_loss = 0.0 val_loss = 0.0 # placeholders fold_preds = np.zeros((len(data_valid), 5)) ##### TRAINING # switch regime model.train() # loop through batches for batch_i, data in enumerate(train_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) optimizer.zero_grad() # forward and backward pass with torch.set_grad_enabled(True): preds = model(inputs) loss = criterion(preds, labels) loss.backward() optimizer.step() # compute loss trn_loss += loss.item() * inputs.size(0) ##### INFERENCE # switch regime model.eval() # loop through batches for batch_i, data in enumerate(valid_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) # compute predictions with torch.set_grad_enabled(False): preds = model(inputs).detach() fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] = preds.cpu().numpy() # compute loss loss = criterion(preds, labels) val_loss += loss.item() * inputs.size(0) # save predictions oof_preds = fold_preds # scheduler step scheduler.step() ##### EVALUATION # evaluate performance fold_preds_round = fold_preds.argmax(axis = 1) val_kappa = metrics.cohen_kappa_score(data_valid[&#39;diagnosis&#39;], fold_preds_round.astype(&#39;int&#39;), weights = &#39;quadratic&#39;) # save perfoirmance values val_kappas.append(val_kappa) val_losses.append(val_loss / len(data_valid)) trn_losses.append(trn_loss / len(data_train)) ##### EARLY STOPPING # display info print(&#39;- epoch {}/{} | lr = {} | trn_loss = {:.4f} | val_loss = {:.4f} | val_kappa = {:.4f} | {:.2f} min&#39;.format( epoch + 1, max_epochs, scheduler.get_lr()[len(scheduler.get_lr()) - 1], trn_loss / len(data_train), val_loss / len(data_valid), val_kappa, (time.time() - epoch_start) / 60)) # check if there is any improvement if epoch &gt; 0: if val_kappas[epoch] &lt; val_kappas[epoch - bad_epochs - 1]: bad_epochs += 1 else: bad_epochs = 0 # save model weights if improvement if bad_epochs == 0: oof_preds_best = oof_preds.copy() torch.save(model.state_dict(), &#39;../models/model_{}.bin&#39;.format(model_name)) # break if early stop if bad_epochs == early_stop: print(&#39;Early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # break if max epochs if epoch == (max_epochs - 1): print(&#39;Did not met early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # load best predictions oof_preds = oof_preds_best # print performance print(&#39;&#39;) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.7140 | val_loss = 1.3364 | val_kappa = 0.7268 | 30.03 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.6447 | val_loss = 1.0670 | val_kappa = 0.8442 | 27.19 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.6203 | val_loss = 0.7667 | val_kappa = 0.7992 | 27.21 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.6020 | val_loss = 0.7472 | val_kappa = 0.8245 | 27.68 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5899 | val_loss = 0.7720 | val_kappa = 0.8541 | 29.42 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5542 | val_loss = 0.9255 | val_kappa = 0.8682 | 29.33 min - epoch 7/15 | lr = 0.0005 | trn_loss = 0.5424 | val_loss = 0.8917 | val_kappa = 0.8763 | 29.91 min - epoch 8/15 | lr = 0.0005 | trn_loss = 0.5359 | val_loss = 0.9555 | val_kappa = 0.8661 | 30.70 min - epoch 9/15 | lr = 0.0005 | trn_loss = 0.5252 | val_loss = 0.8642 | val_kappa = 0.8778 | 28.76 min - epoch 10/15 | lr = 0.000125 | trn_loss = 0.5184 | val_loss = 1.1568 | val_kappa = 0.8403 | 31.14 min - epoch 11/15 | lr = 0.00025 | trn_loss = 0.4974 | val_loss = 0.9464 | val_kappa = 0.8784 | 28.00 min - epoch 12/15 | lr = 0.00025 | trn_loss = 0.4874 | val_loss = 0.9043 | val_kappa = 0.8820 | 27.50 min - epoch 13/15 | lr = 0.00025 | trn_loss = 0.4820 | val_loss = 0.7924 | val_kappa = 0.8775 | 27.36 min - epoch 14/15 | lr = 0.00025 | trn_loss = 0.4758 | val_loss = 0.9300 | val_kappa = 0.8761 | 27.33 min - epoch 15/15 | lr = 6.25e-05 | trn_loss = 0.4693 | val_loss = 0.9109 | val_kappa = 0.8803 | 27.51 min Did not met early stopping. Best results: loss = 0.7472, kappa = 0.8245 (epoch 4) Finished in 429.16 minutes . Training on the Kaggle GPU-enabled machine took us about 7 hours! Let&#39;s visualize the training and validation loss dynamics. . #collapse-show # plot size fig = plt.figure(figsize = (15, 5)) # plot loss dynamics plt.subplot(1, 2, 1) plt.plot(trn_losses, &#39;red&#39;, label = &#39;Training&#39;) plt.plot(val_losses, &#39;green&#39;, label = &#39;Validation&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() # plot kappa dynamics plt.subplot(1, 2, 2) plt.plot(val_kappas, &#39;blue&#39;, label = &#39;Kappa&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Kappa&#39;) plt.legend() . . . The cross-entropy loss on the validation set reaches minimum already after 3 epochs. At the same time, kappa continues to increase up to the 15th epoch. Since we use kappa to evaluate the quality of our solution, we save weights after 15 epochs. . We also construct a confusion matrix of the trained model. The numbers in the cells are percentages. According to the results, the model does a poor job in distinguishing the mild and moderate stages of DR: 86% of images with mild DR are classified as moderate. The best performance is observed for healthy patients. Overall, we see that the model tends to confuse nearby severity stages but rarely misclassifies the proliferate and mild stages. . . Fine-tuning . Fine-tuning on the target data is performed within 4-fold cross-validation. To ensure that we have enough examples of each class, we perform cross-validation with stratification. . On each iteration, we instantiate the EfficientNet B4 model with the same architecture as in the previous section. Next, we load the saved weights from the model pre-trained on the source data. We freeze weights on all network layers except for the last fully-connected layer. The weights in this layer are fine-tuned. As on the pre-training stage, we use Adam optimizer and implement a learning rate scheduler. We also track performance on the validation folds and stop training if kappa does not increase for 5 consecutive epochs. . The process is repeated for each of the 4 folds, and the best model weights are saved for each combination of the training folds. . The init_model() is updated to load the weights saved on the pre-training stage and freeze the first layers of the network in the training regime. . #collapse-show # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True, trn_layers = 2): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) model.load_state_dict(torch.load(&#39;../models/model_{}.bin&#39;.format(model_name, 1))) # freeze first layers for child in list(model.children())[:-trn_layers]: for param in child.parameters(): param.requires_grad = False ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) model.load_state_dict(torch.load(&#39;../models/model_{}.bin&#39;.format(model_name, 1))) # freeze all layers for param in model.parameters(): param.requires_grad = False return model # check architecture model = init_model() . . The training loop is now wrapped into a cross-validation loop. . #collapse-hide ##### VALIDATION SETTINGS # no. folds num_folds = 4 # creating splits skf = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = seed) splits = list(skf.split(train[&#39;id_code&#39;], train[&#39;diagnosis&#39;])) # placeholders oof_preds = np.zeros((len(train), 1)) # timer cv_start = time.time() ##### PARAMETERS # loss function criterion = nn.CrossEntropyLoss() # epochs max_epochs = 15 early_stop = 5 # learning rates eta = 1e-3 # scheduler step = 5 gamma = 0.5 ##### CROSS-VALIDATION LOOP for fold in tqdm(range(num_folds)): ####### DATA PREPARATION # display information print(&#39;-&#39; * 30) print(&#39;FOLD {}/{}&#39;.format(fold + 1, num_folds)) print(&#39;-&#39; * 30) # load splits data_train = train.iloc[splits[fold][0]].reset_index(drop = True) data_valid = train.iloc[splits[fold][1]].reset_index(drop = True) # create datasets train_dataset = EyeTrainData(data = data_train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = train_trans) valid_dataset = EyeTrainData(data = data_valid, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = valid_trans) # create data loaders train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4) valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) ####### MODEL PREPARATION # placeholders val_kappas = [] val_losses = [] trn_losses = [] bad_epochs = 0 # load best OOF predictions if fold &gt; 0: oof_preds = oof_preds_best.copy() # initialize and send to GPU model = init_model(train = True) model = model.to(device) # optimizer optimizer = optim.Adam(model._fc.parameters(), lr = eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) ####### TRAINING AND VALIDATION LOOP for epoch in range(max_epochs): ##### PREPARATION # timer epoch_start = time.time() # reset losses trn_loss = 0.0 val_loss = 0.0 # placeholders fold_preds = np.zeros((len(data_valid), 1)) ##### TRAINING # switch regime model.train() # loop through batches for batch_i, data in enumerate(train_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) optimizer.zero_grad() # forward and backward pass with torch.set_grad_enabled(True): preds = model(inputs) loss = criterion(preds, labels) loss.backward() optimizer.step() # compute loss trn_loss += loss.item() * inputs.size(0) ##### INFERENCE # initialize model.eval() # loop through batches for batch_i, data in enumerate(valid_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) # compute predictions with torch.set_grad_enabled(False): preds = model(inputs).detach() _, class_preds = preds.topk(1) fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] = class_preds.cpu().numpy() # compute loss loss = criterion(preds, labels) val_loss += loss.item() * inputs.size(0) # save predictions oof_preds[splits[fold][1]] = fold_preds # scheduler step scheduler.step() ##### EVALUATION # evaluate performance fold_preds_round = fold_preds val_kappa = metrics.cohen_kappa_score(data_valid[&#39;diagnosis&#39;], fold_preds_round.astype(&#39;int&#39;), weights = &#39;quadratic&#39;) # save perfoirmance values val_kappas.append(val_kappa) val_losses.append(val_loss / len(data_valid)) trn_losses.append(trn_loss / len(data_train)) ##### EARLY STOPPING # display info print(&#39;- epoch {}/{} | lr = {} | trn_loss = {:.4f} | val_loss = {:.4f} | val_kappa = {:.4f} | {:.2f} min&#39;.format( epoch + 1, max_epochs, scheduler.get_lr()[len(scheduler.get_lr()) - 1], trn_loss / len(data_train), val_loss / len(data_valid), val_kappa, (time.time() - epoch_start) / 60)) # check if there is any improvement if epoch &gt; 0: if val_kappas[epoch] &lt; val_kappas[epoch - bad_epochs - 1]: bad_epochs += 1 else: bad_epochs = 0 # save model weights if improvement if bad_epochs == 0: oof_preds_best = oof_preds.copy() torch.save(model.state_dict(), &#39;../models/model_{}_fold{}.bin&#39;.format(model_name, fold + 1)) # break if early stop if bad_epochs == early_stop: print(&#39;Early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # break if max epochs if epoch == (max_epochs - 1): print(&#39;Did not meet early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # load best predictions oof_preds = oof_preds_best # print performance print(&#39;&#39;) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . FOLD 1/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6279 | val_loss = 0.5368 | val_kappa = 0.8725 | 7.30 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5699 | val_loss = 0.5402 | val_kappa = 0.8662 | 7.25 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5572 | val_loss = 0.5380 | val_kappa = 0.8631 | 7.31 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5482 | val_loss = 0.5357 | val_kappa = 0.8590 | 7.29 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5658 | val_loss = 0.5357 | val_kappa = 0.8613 | 7.25 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5537 | val_loss = 0.5346 | val_kappa = 0.8604 | 7.28 min Early stopping. Best results: loss = 0.5346, kappa = 0.8604 (epoch 6) FOLD 2/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6535 | val_loss = 0.5295 | val_kappa = 0.8767 | 7.24 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5691 | val_loss = 0.5158 | val_kappa = 0.8717 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5645 | val_loss = 0.5136 | val_kappa = 0.8732 | 7.23 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5592 | val_loss = 0.5151 | val_kappa = 0.8705 | 7.26 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5530 | val_loss = 0.5213 | val_kappa = 0.8686 | 7.27 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5499 | val_loss = 0.5143 | val_kappa = 0.8733 | 7.21 min Early stopping. Best results: loss = 0.5136, kappa = 0.8732 (epoch 3) FOLD 3/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6503 | val_loss = 0.5286 | val_kappa = 0.8937 | 7.16 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5916 | val_loss = 0.5166 | val_kappa = 0.8895 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5702 | val_loss = 0.5115 | val_kappa = 0.8834 | 7.34 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5606 | val_loss = 0.5133 | val_kappa = 0.8829 | 7.44 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5640 | val_loss = 0.5081 | val_kappa = 0.8880 | 7.28 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5515 | val_loss = 0.5109 | val_kappa = 0.8871 | 7.20 min Early stopping. Best results: loss = 0.5081, kappa = 0.8880 (epoch 5) FOLD 4/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6379 | val_loss = 0.5563 | val_kappa = 0.8595 | 7.18 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5718 | val_loss = 0.5423 | val_kappa = 0.8610 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5590 | val_loss = 0.5433 | val_kappa = 0.8587 | 7.19 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5554 | val_loss = 0.5433 | val_kappa = 0.8579 | 7.17 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5476 | val_loss = 0.5393 | val_kappa = 0.8608 | 7.18 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5509 | val_loss = 0.5331 | val_kappa = 0.8610 | 7.32 min - epoch 7/15 | lr = 0.0005 | trn_loss = 0.5532 | val_loss = 0.5309 | val_kappa = 0.8567 | 7.28 min Early stopping. Best results: loss = 0.5309, kappa = 0.8567 (epoch 7) Finished in 181.29 minutes . The model converges rather quickly. The best validation performance is obtained after 3 to 7 training epochs depending on a fold. . Let&#39;s look at the confusion matrix. The matrix illustrates the advantages of the fine-tuned model over the pre-trained CNN and indicates a better performance in classifying mild stages of the DR. However, we also observe that the model classifies too many examples as moderate (class = 2). . #collapse-hide # construct confusion matrx oof_preds_round = oof_preds.copy() cm = confusion_matrix(train[&#39;diagnosis&#39;], oof_preds_round) cm = cm.astype(&#39;float&#39;) / cm.sum(axis = 1)[:, np.newaxis] annot = np.around(cm, 2) # plot matrix fig, ax = plt.subplots(figsize = (8, 6)) sns.heatmap(cm, cmap = &#39;Blues&#39;, annot = annot, lw = 0.5) ax.set_xlabel(&#39;Prediction&#39;) ax.set_ylabel(&#39;Ground Truth&#39;) ax.set_aspect(&#39;equal&#39;) . . . Inference . Let&#39;s now produce some predictions for the test set! . We aggregate predictions from the models trained during the cross-validation loop. To do so, we extract class scores from the last fully-connected layer and define class predictions as the classes with the maximal score. Next, we average predictions of the 4 networks trained on different combinations of the training folds. . #collapse-hide ############ TRANSFORMATIONS # parameters batch_size = 25 image_size = 256 # test transformations test_trans = transforms.Compose([transforms.ToPILImage(), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor() ]) ############ DATA LOADER # create dataset test_dataset = EyeTestData(data = test, directory = &#39;../input/aptos2019-blindness-detection/test_images&#39;, transform = test_trans) # create data loader test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) ############ MODEL ARCHITECTURE # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True, trn_layers = 2): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) # freeze first layers for child in list(model.children())[:-trn_layers]: for param in child.parameters(): param.requires_grad = False ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_name(&#39;efficientnet-b4&#39;) model._fc = nn.Linear(model._fc.in_features, 5) # freeze all layers for param in model.parameters(): param.requires_grad = False ### return model return model # check architecture model = init_model(train = False) . . We also use test-time augmentations by creating 4 versions of the test images with random augmentations (horizontal and vertical flips) and average predictions over the image variants. The final prediction is an average of 4 models times 4 image variants. . #collapse-show # validation settings num_folds = 4 tta_times = 4 # placeholders test_preds = np.zeros((len(test), num_folds)) cv_start = time.time() # prediction loop for fold in tqdm(range(num_folds)): # load model and sent to GPU model = init_model(train = False) model.load_state_dict(torch.load(&#39;../models/model_{}_fold{}.bin&#39;.format(model_name, fold + 1))) model = model.to(device) model.eval() # placeholder fold_preds = np.zeros((len(test), 1)) # loop through batches for _ in range(tta_times): for batch_i, data in enumerate(test_loader): inputs = data[&#39;image&#39;] inputs = inputs.to(device, dtype = torch.float) preds = model(inputs).detach() _, class_preds = preds.topk(1) fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] += class_preds.cpu().numpy() fold_preds = fold_preds / tta_times # aggregate predictions test_preds[:, fold] = fold_preds.reshape(-1) # print performance test_preds_df = pd.DataFrame(test_preds.copy()) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . Let&#39;s have a look at the distribution of predictions: . #collapse-hide # show predictions print(&#39;-&#39; * 45) print(&#39;PREDICTIONS&#39;) print(&#39;-&#39; * 45) print(test_preds_df.head()) # show correlation print(&#39;-&#39; * 45) print(&#39;CORRELATION MATRIX&#39;) print(&#39;-&#39; * 45) print(np.round(test_preds_df.corr(), 4)) print(&#39;Mean correlation = &#39; + str(np.round(np.mean(np.mean(test_preds_df.corr())), 4))) # show stats print(&#39;-&#39; * 45) print(&#39;SUMMARY STATS&#39;) print(&#39;-&#39; * 45) print(test_preds_df.describe()) # show prediction distribution print(&#39;-&#39; * 45) print(&#39;ROUNDED PREDICTIONS&#39;) print(&#39;-&#39; * 45) for f in range(num_folds): print(np.round(test_preds_df[f]).astype(&#39;int&#39;).value_counts(normalize = True)) print(&#39;-&#39; * 45) # plot densities test_preds_df.plot.kde() . . PREDICTIONS 0 1 2 3 0 2.0 2.0 2.0 1.5 1 2.0 2.5 2.0 2.0 2 2.0 2.0 2.0 2.0 3 2.0 3.0 2.0 2.0 4 2.0 2.0 2.0 2.0 CORRELATION MATRIX 0 1 2 3 0 1.0000 0.9624 0.9573 0.9534 1 0.9624 1.0000 0.9686 0.9490 2 0.9573 0.9686 1.0000 0.9478 3 0.9534 0.9490 0.9478 1.0000 Mean correlation = 0.9673 SUMMARY STATS 0 1 2 3 count 1928.000000 1928.000000 1928.000000 1928.000000 mean 1.686722 1.710322 1.715249 1.650156 std 0.972716 0.973031 0.970417 0.960637 min 0.000000 0.000000 0.000000 0.000000 25% 1.500000 2.000000 2.000000 1.375000 50% 2.000000 2.000000 2.000000 2.000000 75% 2.000000 2.000000 2.000000 2.000000 max 4.000000 4.000000 4.000000 4.000000 ROUNDED PREDICTIONS 2 0.665456 0 0.200207 1 0.047199 3 0.044087 4 0.043050 Name: 0, dtype: float64 2 0.670124 0 0.195021 4 0.046162 3 0.045124 1 0.043568 Name: 1, dtype: float64 2 0.678942 0 0.195021 4 0.048755 1 0.038900 3 0.038382 Name: 2, dtype: float64 2 0.685685 0 0.207988 1 0.042012 4 0.040975 3 0.023340 Name: 3, dtype: float64 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8e403032d0&gt; . The model classifies a lot of images as moderate DR. To reduce the number of such examples, we can change thresholds used to round the averaged predictions into classes. We use the following vector of thresholds: [0.5, 1.75, 2.25, 3.5]. The final prediction is set to zero if the average value is below 0.5; set to one if the average value lies in [0.5, 1.75), etc. This reduces the share of images classified as moderate DR. . #collapse-show # aggregate predictions test_preds = test_preds_df.mean(axis = 1).values # set cutoffs coef = [0.5, 1.75, 2.25, 3.5] # rounding for i, pred in enumerate(test_preds): if pred &lt; coef[0]: test_preds[i] = 0 elif pred &gt;= coef[0] and pred &lt; coef[1]: test_preds[i] = 1 elif pred &gt;= coef[1] and pred &lt; coef[2]: test_preds[i] = 2 elif pred &gt;= coef[2] and pred &lt; coef[3]: test_preds[i] = 3 else: test_preds[i] = 4 . . We are done! We can now export test_preds as csv and submit it to the competition. . 5. Closing words . This blogpost provides a complete walkthrough on the project on detecting blindness in the retina images using CNNs. We use image preprocessing to reduce discrepancies across images taken in different clinics, apply transfer learning to leverage knowledge from larger data sets and implement different techniques to improve performance. . If you are still reading this post, you might be wondering about ways to further improve the solution. There are multiple options. First, employing a larger network architecture and increasing the number of training epochs on the pre-training stage has a high potential for a better performance. At the same time, this would require more computing power, which might not be optimal considering the possible use of the automated retina image classification in practice. . Second, image preprocessing can be further improved. During the refinement process, the largest performance gains were attributed to different preprocessing steps. This is a more efficient way to further improve the performance. . Finally, the best solutions of other competitors rely on ensembles of CNNs using different image sizes and/or architectures. Incorporating multiple heterogeneous models and blending their predictions could also improve the proposed solution. Ensembling predictions of models similar to the one discussed in this post is what helped me to place in the top 9% of the leaderboard. .",
            "url": "https://kozodoi.me/python/deep%20learning/computer%20vision/competitions/2020/07/11/blindness-detection.html",
            "relUrl": "/python/deep%20learning/computer%20vision/competitions/2020/07/11/blindness-detection.html",
            "date": " â€¢ Jul 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Algorithmic Fairness in R",
            "content": "1. Overview . How to measure fairness of a machine learning model? . To date, a number of algorithmic fairness metrics have been proposed. Demographic parity, proportional parity and equalized odds are among the most commonly used metrics to evaluate fairness across sensitive groups in binary classification problems. Multiple other metrics have been proposed based on performance measures extracted from the confusion matrix (e.g., false positive rate parity, false negative rate parity). . Together with Tibor V. Varga, we developed fairness package for R. The package provides tools to calculate algorithmic fairness metrics across different sensitive groups. It also provides opportunities to visualize and compare other prediction metrics between the groups. . This blogpost provides a tutorial on using the fairness package on a COMPAS data set. The blogpost is also available as an interactive Kaggle notebook. The package is published on CRAN and GitHub. . The package provides functions to compute the commonly used metrics of algorithmic fairness: . Demographic parity | Proportional parity | Equalized odds | Predictive rate parity | . In addition, the following comparisons are also implemented: . False positive rate parity | False negative rate parity | Accuracy parity | Negative predictive value parity | Specificity parity | ROC AUC comparison | MCC comparison | . 2. Installation . You can install the latest stable package version from CRAN by running: . #collapse-show install.packages(&#39;fairness&#39;) library(fairness) . . Installing package into â€˜/usr/local/lib/R/site-libraryâ€™ (as â€˜libâ€™ is unspecified) . You may also install the development version from Github: . #collapse-show devtools::install_github(&#39;kozodoi/fairness&#39;) library(fairness) . . 3. Data description . The package includes two exemplary data sets to study fairness: compas and germancredit. . compas . In this tutorial, you will be able to use a simplified version of the landmark COMPAS data set containing the criminal history of defendants from Broward County. You can read more about the data here. To load the data set, all you need to do is: . #collapse-show data(&#39;compas&#39;) head(compas) . . A data.frame: 6 Ã— 9 Two_yr_RecidivismNumber_of_PriorsAge_Above_FourtyFiveAge_Below_TwentyFiveFemaleMisdemeanorethnicityprobabilitypredicted . &lt;fct&gt;&lt;dbl&gt;&lt;fct&gt;&lt;fct&gt;&lt;fct&gt;&lt;fct&gt;&lt;fct&gt;&lt;dbl&gt;&lt;dbl&gt; . 4no | -0.6843578 | no | no | Male | yes | Other | 0.3151557 | 0 | . 5yes | 2.2668817 | no | no | Male | no | Caucasian | 0.8854616 | 1 | . 7no | -0.6843578 | no | no | Female | yes | Caucasian | 0.2552680 | 0 | . 11no | -0.6843578 | no | no | Male | no | African_American | 0.4173908 | 0 | . 14no | -0.6843578 | no | no | Male | yes | Hispanic | 0.3200982 | 0 | . 24no | -0.6843578 | no | no | Male | yes | Other | 0.3151557 | 0 | . The data set contains nine variables. The outcome variable is Two_yr_Recidivism, which is a binary indicator showing whether an individual commited a crime within the two-year period. The data also includes features on prior criminal record (Number_of_Priors, Misdemeanor) and other features describing age (Age_Above_FourtyFive, Age_Below_TwentyFive), sex and ethnicity (Female, ethnicity). . You donâ€™t really need to delve into the data much. To simplify illustration, we have already trained a classifier that uses all available features to predict Two_yr_Recidivism and concatenated the predicted probabilities (probability) and predicted classes (predicted) to the data frame. You will be able to use these columns with predictions directly in your analysis to test different metrics before using a real model. . germancredit . The second data set included in the package is a credit scoring data set labeled as germancredit. The data set includes 20 features describing the loan applicants and an outcome variable named BAD, which is a binary indicator showing whether the applicant defaulted on a loan. Similarly to the compas data set, this data also includes two columns with model predictions named probability and predicted. . Feel free to play with this data as well. You can load it with: . #collapse-show data(&#39;germancredit&#39;) . . 4. Train a classifier . For the purpose of this tutorial, we will train two new models using different sets of features: . model that uses all features as input | model that uses all features except for ethnicity | . We partition the COMPAS data into training and validation subsets and use logistic regression as base classifier. . #collapse-show # extract data compas &lt;- fairness::compas df &lt;- compas[, !(colnames(compas) %in% c(&#39;probability&#39;, &#39;predicted&#39;))] # partitioning params set.seed(77) val_percent &lt;- 0.3 val_idx &lt;- sample(1:nrow(df))[1:round(nrow(df) * val_percent)] # partition the data df_train &lt;- df[-val_idx, ] df_valid &lt;- df[ val_idx, ] # check dim print(nrow(df_train)) print(nrow(df_valid)) . . [1] 4320 [1] 1852 . #collapse-show # fit logit models model1 &lt;- glm(Two_yr_Recidivism ~ ., data = df_train, family = binomial(link = &#39;logit&#39;)) model2 &lt;- glm(Two_yr_Recidivism ~ . -ethnicity, data = df_train, family = binomial(link = &#39;logit&#39;)) . . Let&#39;s append model predictions to the validation set. Later, we will evaluate fairness of the two models based on these predictions. . #collapse-show # produce predictions df_valid$prob_1 &lt;- predict(model1, df_valid, type = &#39;response&#39;) df_valid$prob_2 &lt;- predict(model2, df_valid, type = &#39;response&#39;) head(df_valid) . . A data.frame: 6 Ã— 9 Two_yr_RecidivismNumber_of_PriorsAge_Above_FourtyFiveAge_Below_TwentyFiveFemaleMisdemeanorethnicityprob_1prob_2 . &lt;fct&gt;&lt;dbl&gt;&lt;fct&gt;&lt;fct&gt;&lt;fct&gt;&lt;fct&gt;&lt;fct&gt;&lt;dbl&gt;&lt;dbl&gt; . 1185no | -0.6843578 | no | no | Male | no | African_American | 0.3678771 | 0.3481577 | . 5535no | 2.0560789 | no | no | Male | no | Hispanic | 0.8024180 | 0.8347748 | . 6054yes | -0.4735549 | no | yes | Male | no | African_American | 0.5895862 | 0.5730560 | . 2320no | -0.6843578 | yes | no | Male | no | African_American | 0.2395689 | 0.2218929 | . 3458yes | 0.5804592 | no | no | Male | no | Caucasian | 0.5915503 | 0.6010710 | . 2114no | -0.6843578 | no | no | Female | yes | Caucasian | 0.2480556 | 0.2496530 | . 5. Algorithmic fairness metrics . The package currently includes nine fairness metrics and two other performance comparisons. Many of these metrics are mutually exclusive: results from a given classification problem most often cannot be fair in terms of all group fairness metrics. Depending on a context, it is important to select an appropriate metric to evaluate fairness. . Below, we intrdocue functions that are used to compute the implemented metrics. Every function has a similar set of arguments: . data: data.frame with the features and predictions | outcome: name of the outcome variable | group: name of the sensitive group, which needs to be a factor variable included in the data.frame. | base: name of the base group (factor level of group) that serves as a base for fairness metrics | . We also need to supply model predictions. Depending on a metric, we need to provide either porbabilistic predictions as probs or class predictions as preds. The model predictions can be appended to the original data.frame or provided as a vector. In this tutorial, we will use probabilistic predctions with all functions. . When working with probabilistic predictions, some metrics also require a cutoff value to convert probabilities into class precictions supplied as cutoff. Finally, we also need to specifiy factor levels to indicate the reference classes using the preds_levels argument. The first level refers to the base class, whereas the second level indicates the predicted class for which probabilities are provided. . An outlook on the confusion matrix . Most fairness metrics are calculated based on a confusion matrix produced by a classification model. The confusion matrix is comprised of four distinct classes: . True positives (TP): the true class is positive and the prediction is positive (correct classification) | False positives (FP): the true class is negative and the prediction is positive (incorrect classification) | True negatives (TN): the true class is negative and the prediction is negative (correct classification) | False negatives (FN): the true class is positive and the prediction is negative (incorrect classification) | . The fairness metrics are calculated by comparing one or more of these measures computed for different sensitive subgroups (e.g., male and female). For a detailed overview of various measures coming from the confusion matrix and precise definitions, please click here or here. . Predictive rate parity . Let&#39;s demonstrate the fairness pipeline using predictive rate parity as an example. Predictive rate parity is achieved if the precisions (or positive predictive values) in the subgroups are close to each other. The precision stands for the number of the true positives divided by the total number of examples predicted positive within a group. . Formula: TP / (TP + FP) . Let&#39;s compute predictive rate parity for the first model that uses all features: . #collapse-show res1 &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;Caucasian&#39;) res1$Metric . . A matrix: 2 Ã— 6 of type dbl CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Precision0.6863158 | 0.6681223 | 0.7857143 | 0.6967213 | 0.6666667 | 0.6489362 | . Predictive Rate Parity1.0000000 | 0.9734910 | 1.1448291 | 1.0151614 | 0.9713701 | 0.9455358 | . The first row shows the raw precision values for all ethnicities. The second row displays the relative precisions compared to Caucasian defendants. . In a perfect world, all predictive rate parities should be equal to one, which would mean that the precision in every group is the same as in the base group. In practice, values are going to be different. The paritiy above one indicates that precision in this group is relatively higher, whereas a lower parity implies a lower precision. Observing a large variance in parities should hint us that the model is not performing equally well for different sensitive groups. . If the other ethnic group is set as a base group (e.g. Hispanic), the raw precision values do not change, only the relative metrics: . #collapse-show res1h &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;Hispanic&#39;) res1h$Metric . . A matrix: 2 Ã— 6 of type dbl HispanicCaucasianAfrican_AmericanAsianNative_AmericanOther . Precision0.6967213 | 0.6863158 | 0.6681223 | 0.7857143 | 0.6666667 | 0.6489362 | . Predictive Rate Parity1.0000000 | 0.9850650 | 0.9589520 | 1.1277311 | 0.9568627 | 0.9314143 | . Overall, results suggest that the model precision varies between 0.6489 and 0.7857. Apart from the &quot;other&quot; category, the lowest precision is observed for African-American defendants. This implies that there are more cases where the model mistakingly predicts that a person will commit a crime among African-Americans than among, e.g., Asian defendants. . A standard output of every fairness metric function includes a barchart that visualizes the relative metrics for all subgroups: . #collapse-show res1h$Metric_plot . . Some fairness metrics do not require probabilistic predictions and can work with class predictions. When predicted probabilities are supplied, an extra density plot will be output displaying the distributions of probabilities of all subgroups and the user-defined cutoff: . #collapse-show res1h$Probability_plot . . Let&#39;s now compare the results to the second model that does not use ethnicity as a feature: . #collapse-show # model 2 res2 &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_2&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;Caucasian&#39;) res2$Metric . . A matrix: 2 Ã— 6 of type dbl CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Precision0.690678 | 0.6609808 | 0.8181818 | 0.706422 | 0.6666667 | 0.6625000 | . Predictive Rate Parity1.000000 | 0.9570029 | 1.1846068 | 1.022795 | 0.9652352 | 0.9592025 | . We can see two things. . First, excluding ethnicity from the features slightly increases precision for African-American defendants but results in a lower precision for a number of other groups. This illustrates that improving a model for one group may cost a fall in the predictive performance for the general population. Depending on the context, it is a task of a decision-maker to decide what is best. . Second, excluding ethnicity does not align the predictive rate parities substantially closer to one. This illustrates another important research finding: removing a sensitive variable does not guarantee that a model stops discriminating. Ethnicity correlates with other features and is still implicitly included in the input data. In order to make the classifier more fair, one would neet to consider more sophisticated techniques than simply dropping the sensitive attribute. . In the rest of this tutorial, we will go through the functions that cover the remaining implemented fairness metrics, illustrating the corresponding equations and outputs. You can find more details on each of the fairness metric functions in the package documentation. Please don&#39;t hesitate to use the built-in helper to see further details and examples on the implemented metrics: . #collapse-show ?fairness::pred_rate_parity . . Demographic parity . Demographic parity is one of the most popular fairness indicators in the literature. Demographic parity is achieved if the absolute number of positive predictions in the subgroups are close to each other. This measure does not take true class into consideration and only depends on the model predictions. . Formula: (TP + FP) . #collapse-show res_dem &lt;- dem_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;Caucasian&#39;) res_dem$Metric . . A matrix: 2 Ã— 6 of type dbl CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Positively classified147 | 504.000000 | 2.00000000 | 22.0000000 | 1.000000000 | 10.00000000 | . Demographic Parity 1 | 3.428571 | 0.01360544 | 0.1496599 | 0.006802721 | 0.06802721 | . #collapse-show res_dem$Metric_plot . . Of course, comparing the absolute number of positive predictions will show a high disparity when the number of cases within each group is different, which artificially boosts the disparity. This is true in our case: . #collapse-show table(df_valid$ethnicity) . . Caucasian African_American Asian Hispanic 622 962 16 144 Native_American Other 4 104 . To address this, we can use proportional parity. . Proportional parity . Proportional parity is very similar to demographic parity but modifies it to address the issue discussed above. Proportional parity is achieved if the proportion of positive predictions in the subgroups are close to each other. Similar to the demographic parity, this measure also does not depend on the true labels. . Formula: (TP + FP) / (TP + FP + TN + FN) . #collapse-show res_prop &lt;- prop_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;Caucasian&#39;) res_prop$Metric . . A matrix: 2 Ã— 6 of type dbl CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Proportion0.2363344 | 0.5239085 | 0.1250000 | 0.1527778 | 0.250000 | 0.09615385 | . Proportional Parity1.0000000 | 2.2168102 | 0.5289116 | 0.6464475 | 1.057823 | 0.40685505 | . #collapse-show res_prop$Metric_plot . . The proportional parity still shows that African-American defendants are treated unfairly by our model. At the same time, the disparity is lower compared to the one observed with the demographic parity. . All the remaining fairness metrics account for both model predictions and the true labels. . Equalized odds . Equalized odds are achieved if the sensitivities in the subgroups are close to each other. The group-specific sensitivities indicate the number of the true positives divided by the total number of positives in that group. . Formula: TP / (TP + FN) . #collapse-show res_eq &lt;- equal_odds(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;African_American&#39;) res_eq$Metric . . A matrix: 2 Ã— 6 of type dbl African_AmericanCaucasianAsianHispanicNative_AmericanOther . Sensitivity0.6710526 | 0.8423773 | 0.9166667 | 0.9042553 | 1.000000 | 0.983871 | . Equalized odds1.0000000 | 1.2553073 | 1.3660131 | 1.3475177 | 1.490196 | 1.466161 | . Accuracy parity . Accuracy parity is achieved if the accuracies (all accurately classified examples divided by the total number of examples) in the subgroups are close to each other. . Formula: (TP + TN) / (TP + FP + TN + FN) . #collapse-show res_acc &lt;- acc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;African_American&#39;) res_acc$Metric . . A matrix: 2 Ã— 6 of type dbl African_AmericanCaucasianAsianHispanicNative_AmericanOther . Accuracy0.6860707 | 0.6623794 | 0.750000 | 0.6805556 | 0.750000 | 0.6730769 | . Accuracy Parity1.0000000 | 0.9654682 | 1.093182 | 0.9919613 | 1.093182 | 0.9810606 | . False negative rate parity . False negative rate parity is achieved if the false negative rates (the ratio between the number of false negatives and the total number of positives) in the subgroups are close to each other. . Formula: FN / (TP + FN) . #collapse-show res_fnr &lt;- fnr_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;African_American&#39;) res_fnr$Metric . . A matrix: 2 Ã— 6 of type dbl African_AmericanCaucasianAsianHispanicNative_AmericanOther . FNR0.3289474 | 0.1576227 | 0.08333333 | 0.09574468 | 0 | 0.01612903 | . FNR Parity1.0000000 | 0.4791731 | 0.25333333 | 0.29106383 | 0 | 0.04903226 | . False positive rate parity . False positive rate parity is achieved if the false positive rates (the ratio between the number of false positives and the total number of negatives) in the subgroups are close to each other. . Formula: FP / (TN + FP) . #collapse-show res_fpr &lt;- fpr_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;African_American&#39;) res_fpr$Metric . . A matrix: 2 Ã— 6 of type dbl African_AmericanCaucasianAsianHispanicNative_AmericanOther . FPR0.3003953 | 0.6340426 | 0.750000 | 0.740000 | 0.500000 | 0.7857143 | . FPR Parity1.0000000 | 2.1106943 | 2.496711 | 2.463421 | 1.664474 | 2.6156015 | . Negative predictive value parity . Negative predictive value parity is achieved if the negative predictive values in the subgroups are close to each other. The negative predictive value is computed as a ratio between the number of true negatives and the total number of predicted negatives. This function can be considered the â€˜inverseâ€™ of the predictive rate parity. . Formula: TN / (TN + FN) . #collapse-show res_npv &lt;- npv_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;African_American&#39;) res_npv$Metric . . A matrix: 2 Ã— 6 of type dbl African_AmericanCaucasianAsianHispanicNative_AmericanOther . NPV0.702381 | 0.5850340 | 0.5000000 | 0.5909091 | 1.000000 | 0.900000 | . NPV Parity1.000000 | 0.8329298 | 0.7118644 | 0.8412943 | 1.423729 | 1.281356 | . Specificity parity . Specificity parity is achieved if the specificities (the ratio of the number of the true negatives and the total number of negatives) in the subgroups are close to each other. This function can be considered the â€˜inverseâ€™ of the equalized odds. . Formula: TN / (TN + FP) . #collapse-show res_sp &lt;- spec_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;African_American&#39;) res_sp$Metric . . A matrix: 2 Ã— 6 of type dbl African_AmericanCaucasianAsianHispanicNative_AmericanOther . Specificity0.6996047 | 0.3659574 | 0.2500000 | 0.2600000 | 0.5000000 | 0.2142857 | . Specificity Parity1.0000000 | 0.5230917 | 0.3573446 | 0.3716384 | 0.7146893 | 0.3062954 | . Apart from the parity-based metrics presented above, two additional comparisons are implemented: ROC AUC comparison and Matthews correlation coefficient comparison. . ROC AUC comparison . This function calculates ROC AUC and visualizes ROC curves for all subgroups. Note that probabilities must be defined for this function. Also, as ROC evaluates all possible cutoffs, the cutoff argument is excluded from this function. . #collapse-show res_auc &lt;- roc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;Female&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), base = &#39;Male&#39;) res_auc$Metric . . Setting direction: controls &lt; cases Setting direction: controls &lt; cases . A matrix: 2 Ã— 2 of type dbl MaleFemale . ROC AUC0.7221429 | 0.7192349 | . ROC AUC Parity1.0000000 | 0.9959731 | . Apart from the standard outputs, the function also returns ROC curves for each of the subgroups: . #collapse-show res_auc$ROCAUC_plot . . Matthews correlation coefficient comparison . The Matthews correlation coefficient (MCC) takes all four classes of the confusion matrix into consideration. MCC is sometimes referred to as the single most powerful metric in binary classification problems, especially for data with class imbalances. . Formula: (TPÃ—TN-FPÃ—FN)/âˆš((TP+FP)Ã—(TP+FN)Ã—(TN+FP)Ã—(TN+FN)) . #collapse-show res_mcc &lt;- mcc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism&#39;, group = &#39;Female&#39;, probs = &#39;prob_1&#39;, preds_levels = c(&#39;no&#39;,&#39;yes&#39;), cutoff = 0.5, base = &#39;Male&#39;) res_mcc$Metric . . A matrix: 2 Ã— 2 of type dbl MaleFemale . MCC0.3316558 | 0.2893650 | . MCC Parity1.0000000 | 0.8724859 | . 6. Closing words . You have read through the fairness R package tutorial! By now, you should have a solid grip on algorithmic group fairness metrics. . We hope that you will be able to use the R package in your data analysis! Please let me know if you run into any issues while working with the package in the comment window below or on GitHub. Please also feel free to contact the authors if you have any feedback. . Acknowlegments: . Calders, T., &amp; Verwer, S. (2010). Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2), 277-292. | Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2), 153-163. | Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., &amp; Venkatasubramanian, S. (2015, August). Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 259-268). ACM. | Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., &amp; Roth, D. (2018). A comparative study of fairness-enhancing interventions in machine learning. arXiv preprint arXiv:1802.04422. | Zafar, M. B., Valera, I., Gomez Rodriguez, M., &amp; Gummadi, K. P. (2017, April). Fairness beyond disparate treatment &amp; disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web (pp. 1171-1180). International World Wide Web Conferences Steering Committee. | .",
            "url": "https://kozodoi.me/r/fairness/packages/2020/05/01/fairness-tutorial.html",
            "relUrl": "/r/fairness/packages/2020/05/01/fairness-tutorial.html",
            "date": " â€¢ May 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I am Nikita! . ðŸ“š PhD candidate at Humbold University of Berlin | ðŸ’» Data science research associate at Monedo | ðŸ¤– Passionate about machine learning &amp; data science | ðŸ¦ Working on machine learning applications in credit risk analytics | ðŸ… Enjoy participating at data science competitions and hackathons | ðŸ§© Love football, scooters and piano | . Click here to view my CV and check out my profile page at the universityâ€™s webiste. . . . . This website hosts my blog, where I share machine learning tutorials, competition solutions and interesting findings from different data science projects. Check out the feed for the latest posts! . . If you want to chat or see more of my work, connect with me on different platforms: . LinkedIn | GitHub | Kaggle | Google Scholar | ResearchGate | . . I am also happy to chat on other social media: . Twitter | Instagram | Facebook | .",
          "url": "https://kozodoi.me/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Kaggle",
          "content": "I enjoy taking part at machine learning competitions on Kaggle. This page summarizes my achievements. Check out my Kaggle profile to see more. . . Structured data . ðŸ¥ˆ Google Analytics Customer Revenue Prediction: top-2% (2-people team, 3611 teams) | ðŸ¥ˆ IEEE-CIS Fraud Detection: top-3% (2-people team, 6381 teams) | ðŸ¥ˆ Home Credit Default Risk: top-4% (solo, 7190 teams) | ðŸ¥ˆ PLAsTiCC Astronomical Classification: top-5% (3-people team, 1094 teams) | ðŸ¥‰ COVID-19 mRNA Vaccine Degradation Prediction: top-6% (solo, 1636 teams) | ðŸ¥‰ Instant Gratification: top-6% (solo, 1832 teams) | . Computer vision . ðŸ¥‡ SIIM-ISIC Melanoma Classification: top-1% (5-people team, 3314 teams) | ðŸ¥‰ Prostate Cancer Grade Assessment Challenge: top-6% (solo, 1010 teams) | ðŸ¥‰ APTOS 2019 Blindness Detection: top-9% (solo, 2931 teams) | . . Highest user rank . Competitions: 571 out of 149,181 | Discussion: 370 out of 165,322 | .",
          "url": "https://kozodoi.me/kaggle/",
          "relUrl": "/kaggle/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Packages",
          "content": "This page provides software packages developed by me. Follow me on GitHub to see more. . . dptools . Python package with helper functions for data processing and feature engineering | the source code and documentation are available on GitHub and PyPi | . . fairness . R package for computing and visualizing algorithmic fairness metrics | the source code and documentation are available on GitHub and CRAN | comprehensive tutorial is provided in this blogpost | .",
          "url": "https://kozodoi.me/packages/",
          "relUrl": "/packages/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Publications",
          "content": "This page provides a selected list of my publications in academic journals and conference proceedings. Follow me on Google Scholar or ResearchGate to see all of my latest work. . . 2020 . Kozodoi, N., Katsas, P., Lessmann, S., Moreira-Matias, L., &amp; Papakonstantinou, K. (2020). Shallow Self-Learning for Reject Inference in Credit Scoring. In ECML PKDD 2019 Proceedings (pp. 516-532). Springer, Cham. [PDF on ResearchGate] [published version] | . 2019 . Kozodoi, N., Lessmann, S., Papakonstantinou, K., Gatsoulis, Y., &amp; Baesens, B. (2019). A multi-objective approach for profit-driven feature selection in credit scoring. Decision Support Systems, 120, 106-117. [PDF on ResearchGate] [published version] . | Kozodoi, N., Lessmann, S., Baesens, B., &amp; Papakonstantinou, K. (2019). Profit-Oriented Feature Selection in Credit Scoring Applications. In Operations Research 2018 Proceedings (pp. 59-65). Springer, Cham. [PDF on ResearchGate] [published version] . | .",
          "url": "https://kozodoi.me/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://kozodoi.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}